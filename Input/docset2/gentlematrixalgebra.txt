Springer Texts in Statistics

Advisors:
George Casella Stephen Fienberg

Ingram Olkin

Springer Texts in Statistics

Athreya/Lahiri: Measure Theory and Probability Theory
Bilodeau/Brenner: Theory of Multivariate Statistics
Brockwell/Davis: An Introduction to Time Series and Forecasting
Carmona: Statistical Analysis of Financial Data in S-PLUS
Chow/Teicher: Probability Theory: Independence, Interchangeability,

Martingales, Third Edition

Christensen: Advanced Linear Modeling: Multivariate, Time Series, and

Spatial Data; Nonparametric Regression and Response Surface
Maximization, Second Edition

Christensen: Log-Linear Models and Logistic Regression, Second Edition
Christensen: Plane Answers to Complex Questions: The Theory of

Linear Models, Second Edition

Davis: Statistical Methods for the Analysis of Repeated Measurements
Dean/Voss: Design and Analysis of Experiments
Dekking/Kraaikamp/Lopuhaä/Meester: A Modern Introduction to

Probability and Statistics

Durrett: Essentials of Stochastic Processes
Edwards: Introduction to Graphical Modeling, Second Edition
Everitt: An R and S-PLUS Companion to Multivariate Analysis
Gentle:Matrix Algebra: Theory, Computations,  and Applications in Statistics
Ghosh/Delampady/Samanta: An Introduction to Bayesian Analysis
Gut: Probability: A Graduate Course
Heiberger/Holland: Statistical Analysis and Data Display; An Intermediate

Course with Examples in S-PLUS, R, and SAS

Jobson: Applied Multivariate Data Analysis, Volume I: Regression and

Experimental Design

Multivariate Methods

Jobson: Applied Multivariate Data Analysis, Volume II: Categorical and

Karr: Probability
Kulkarni: Modeling, Analysis, Design, and Control of Stochastic Systems
Lange: Applied Probability
Lange: Optimization
Lehmann: Elements of Large Sample Theory
Lehmann/Romano: Testing Statistical Hypotheses, Third Edition
Lehmann/Casella: Theory of Point Estimation, Second Edition
Marin/Robert: Bayesian Core: A Practical Approach to Computational

Bayesian Statistics

Nolan/Speed: Stat Labs: Mathematical Statistics Through Applications
Pitman: Probability
Rawlings/Pantula/Dickey: Applied Regression Analysis

(continued after index)

James E. Gentle

Matrix Algebra
Theory, Computations, and Applications
in Statistics

James E. Gentle
Department of Computational 

and Data Sciences

George Mason University
4400 University Drive
Fairfax, VA 22030-4444
jgentle@gmu.edu

Editorial Board
George Casella
Department of Statistics
University of Florida
Gainesville, FL 32611-8545 Pittsburgh, PA 15213-3890
USA

Stephen Fienberg
Department of Statistics
Carnegie Mellon University Stanford University
Stanford, CA 94305
USA

Ingram Olkin
Department of Statistics

USA

ISBN :978-0-387-70872-0

e-ISBN :978-0-387-70873-7

Library of Congress Control Number: 2007930269

© 2007 Springer Science+Business Media, LLC
All rights reserved. This work may not be translated or copied in whole or in part without the
written permission of the publisher (Springer Science+Business Media, LLC, 233 Spring Street,
New  York, NY, 10013, USA), except  for  brief excerpts  in  connection  with  reviews  or  scholarly
analysis. Use  in  connection  with  any  form  of information  storage  and  retrieval, electronic
adaptation, computer software, or by similar or dissimilar methodology now known or hereafter
developed is forbidden.
The use in this publication of trade names, trademarks, service marks, and similar terms, even if
they are not identified as such, is not to be taken as an expression of opinion as to whether or
not they are subject to proprietary rights.

Printed on acid-free paper.

9 8 7 6 5 4 3 2 1

springer.com

To Mar´ıa

Preface

I began this book as an update of Numerical Linear Algebra for Applications
in Statistics, published by Springer in 1998. There was a modest amount of
new material to add, but I also wanted to supply more of the reasoning behind
the facts about vectors and matrices. I had used material from that text in
some courses, and I had spent a considerable amount of class time proving
assertions made but not proved in that book. As I embarked on this project,
the character of the book began to change markedly. In the previous book,
I apologized for spending 30 pages on the theory and basic facts of linear
algebra before getting on to the main interest: numerical linear algebra. In
the present book, discussion of those basic facts takes up over half of the book.
The orientation and perspective of this book remains numerical linear al-
gebra for applications in statistics. Computational considerations inform the
narrative. There is an emphasis on the areas of matrix analysis that are im-
portant for statisticians, and the kinds of matrices encountered in statistical
applications receive special attention.

This book is divided into three parts plus a set of appendices. The three
parts correspond generally to the three areas of the book’s subtitle — theory,
computations, and applications — although the parts are in a diﬀerent order,
and there is no ﬁrm separation of the topics.

Part I, consisting of Chapters 1 through 7, covers most of the material
in linear algebra needed by statisticians. (The word “matrix” in the title of
the present book may suggest a somewhat more limited domain than “linear
algebra”; but I use the former term only because it seems to be more commonly
used by statisticians and is used more or less synonymously with the latter
term.)

The ﬁrst four chapters cover the basics of vectors and matrices, concen-
trating on topics that are particularly relevant for statistical applications. In
Chapter 4, it is assumed that the reader is generally familiar with the basics of
partial diﬀerentiation of scalar functions. Chapters 5 through 7 begin to take
on more of an applications ﬂavor, as well as beginning to give more consid-
eration to computational methods. Although the details of the computations

viii

Preface

are not covered in those chapters, the topics addressed are oriented more to-
ward computational algorithms. Chapter 5 covers methods for decomposing
matrices into useful factors.

Chapter 6 addresses applications of matrices in setting up and solving
linear systems, including overdetermined systems. We should not confuse sta-
tistical inference with ﬁtting equations to data, although the latter task is
a component of the former activity. In Chapter 6, we address the more me-
chanical aspects of the problem of ﬁtting equations to data. Applications in
statistical data analysis are discussed in Chapter 9. In those applications, we
need to make statements (that is, assumptions) about relevant probability
distributions.

Chapter 7 discusses methods for extracting eigenvalues and eigenvectors.
There are many important details of algorithms for eigenanalysis, but they
are beyond the scope of this book. As with other chapters in Part I, Chap-
ter 7 makes some reference to statistical applications, but it focuses on the
mathematical and mechanical aspects of the problem.

Although the ﬁrst part is on “theory”, the presentation is informal; neither
deﬁnitions nor facts are highlighted by such words as “Deﬁnition”, “Theorem”,
“Lemma”, and so forth. It is assumed that the reader follows the natural
development. Most of the facts have simple proofs, and most proofs are given
” appear to indicate
naturally in the text. No “Proof” and “Q.E.D.” or “
beginning and end; again, it is assumed that the reader is engaged in the
development. For example, on page 270:

If A is nonsingular and symmetric, then A−1 is also symmetric because
(A−1)T = (AT)−1 = A−1.

The ﬁrst part of that sentence could have been stated as a theorem and
given a number, and the last part of the sentence could have been introduced
as the proof, with reference to some previous theorem that the inverse and
transposition operations can be interchanged. (This had already been shown
before page 270 — in an unnumbered theorem of course!)

None of the proofs are original (at least, I don’t think they are), but in most
cases I do not know the original source, or even the source where I ﬁrst saw
them. I would guess that many go back to C. F. Gauss. Most, whether they
are as old as Gauss or not, have appeared somewhere in the work of C. R. Rao.
Some lengthier proofs are only given in outline, but references are given for
the details. Very useful sources of details of the proofs are Harville (1997),
especially for facts relating to applications in linear models, and Horn and
Johnson (1991) for more general topics, especially those relating to stochastic
matrices. The older books by Gantmacher (1959) provide extensive coverage
and often rather novel proofs. These two volumes have been brought back into
print by the American Mathematical Society.

I also sometimes make simple assumptions without stating them explicitly.
For example, I may write “for all i” when i is used as an index to a vector.
I hope it is clear that “for all i” means only “for i that correspond to indices

Preface

ix

of the vector”. Also, my use of an expression generally implies existence. For
example, if “AB” is used to represent a matrix product, it implies that “A
and B are conformable for the multiplication AB”. Occasionally I remind the
reader that I am taking such shortcuts.

The material in Part I, as in the entire book, was built up recursively. In the
ﬁrst pass, I began with some deﬁnitions and followed those with some facts
that are useful in applications. In the second pass, I went back and added
deﬁnitions and additional facts that lead to the results stated in the ﬁrst
pass. The supporting material was added as close to the point where it was
needed as practical and as necessary to form a logical ﬂow. Facts motivated by
additional applications were also included in the second pass. In subsequent
passes, I continued to add supporting material as necessary and to address
the linear algebra for additional areas of application. I sought a bare-bones
presentation that gets across what I considered to be the theory necessary for
most applications in the data sciences. The material chosen for inclusion is
motivated by applications.

Throughout the book, some attention is given to numerical methods for
computing the various quantities discussed. This is in keeping with my be-
lief that statistical computing should be dispersed throughout the statistics
curriculum and statistical literature generally. Thus, unlike in other books
on matrix “theory”, I describe the “modiﬁed” Gram-Schmidt method, rather
than just the “classical” GS. (I put “modiﬁed” and “classical” in quotes be-
cause, to me, GS is MGS. History is interesting, but in computational matters,
I do not care to dwell on the methods of the past.) Also, condition numbers
of matrices are introduced in the “theory” part of the book, rather than just
in the “computational” part. Condition numbers also relate to fundamental
properties of the model and the data.

The diﬀerence between an expression and a computing method is em-
phasized. For example, often we may write the solution to the linear system
Ax = b as A−1b. Although this is the solution (so long as A is square and of
full rank), solving the linear system does not involve computing A−1. We may
write A−1b, but we know we can compute the solution without inverting the
matrix.

“This is an instance of a principle that we will encounter repeatedly:
the form of a mathematical expression and the way the expression
should be evaluated in actual practice may be quite diﬀerent.”

(The statement in quotes appears word for word in several places in the book.)
Standard textbooks on “matrices for statistical applications” emphasize
their uses in the analysis of traditional linear models. This is a large and im-
portant ﬁeld in which real matrices are of interest, and the important kinds of
real matrices include symmetric, positive deﬁnite, projection, and generalized
inverse matrices. This area of application also motivates much of the discussion
in this book. In other areas of statistics, however, there are diﬀerent matrices of
interest, including similarity and dissimilarity matrices, stochastic matrices,

x

Preface

rotation matrices, and matrices arising from graph-theoretic approaches to
data analysis. These matrices have applications in clustering, data mining,
stochastic processes, and graphics; therefore, I describe these matrices and
their special properties. I also discuss the geometry of matrix algebra. This
provides a better intuition of the operations. Homogeneous coordinates and
special operations in IR3 are covered because of their geometrical applications
in statistical graphics.

Part II addresses selected applications in data analysis. Applications are
referred to frequently in Part I, and of course, the choice of topics for coverage
was motivated by applications. The diﬀerence in Part II is in its orientation.
Only “selected” applications in data analysis are addressed; there are ap-
plications of matrix algebra in almost all areas of statistics, including the
theory of estimation, which is touched upon in Chapter 4 of Part I. Certain
types of matrices are more common in statistics, and Chapter 8 discusses in
more detail some of the important types of matrices that arise in data analy-
sis and statistical modeling. Chapter 9 addresses selected applications in data
analysis. The material of Chapter 9 has no obvious deﬁnition that could be
covered in a single chapter (or a single part, or even a single book), so I have
chosen to discuss brieﬂy a wide range of areas. Most of the sections and even
subsections of Chapter 9 are on topics to which entire books are devoted;
however, I do not believe that any single book addresses all of them.

Part III covers some of the important details of numerical computations,
with an emphasis on those for linear algebra. I believe these topics constitute
the most important material for an introductory course in numerical analysis
for statisticians and should be covered in every such course.

Except for speciﬁc computational techniques for optimization, random
number generation, and perhaps symbolic computation, Part III provides the
basic material for a course in statistical computing. All statisticians should
have a passing familiarity with the principles.

Chapter 10 provides some basic information on how data are stored and
manipulated in a computer. Some of this material is rather tedious, but it
is important to have a general understanding of computer arithmetic before
considering computations for linear algebra. Some readers may skip or just
skim Chapter 10, but the reader should be aware that the way the computer
stores numbers and performs computations has far-reaching consequences.
Computer arithmetic diﬀers from ordinary arithmetic in many ways; for ex-
ample, computer arithmetic lacks associativity of addition and multiplication,
and series often converge even when they are not supposed to. (On the com-

puter, a straightforward evaluation of!∞x=1 x converges!)

I emphasize the diﬀerences between the abstract number system IR, called
the reals, and the computer number system IF, the ﬂoating-point numbers
unfortunately also often called “real”. Table 10.3 on page 400 summarizes
some of these diﬀerences. All statisticians should be aware of the eﬀects of
these diﬀerences. I also discuss the diﬀerences between ZZ, the abstract number
system called the integers, and the computer number system II, the ﬁxed-point

Preface

xi

numbers. (Appendix A provides deﬁnitions for this and other notation that I
use.)

Chapter 10 also covers some of the fundamentals of algorithms, such as
iterations, recursion, and convergence. It also discusses software development.
Software issues are revisited in Chapter 12.

While Chapter 10 deals with general issues in numerical analysis, Chap-
ter 11 addresses speciﬁc issues in numerical methods for computations in linear
algebra.

Chapter 12 provides a brief introduction to software available for com-
putations with linear systems. Some speciﬁc systems mentioned include the
IMSLTM libraries for Fortran and C, Octave or MATLAB R⃝ (or Matlab R⃝),
and R or S-PLUS R⃝ (or S-Plus R⃝). All of these systems are easy to use, and
the best way to learn them is to begin using them for simple problems. I do
not use any particular software system in the book, but in some exercises, and
particularly in Part III, I do assume the ability to program in either Fortran
or C and the availability of either R or S-Plus, Octave or Matlab, and Maple R⃝
or Mathematica R⃝. My own preferences for software systems are Fortran and
R, and occasionally these preferences manifest themselves in the text.

Appendix A collects the notation used in this book. It is generally “stan-
dard” notation, but one thing the reader must become accustomed to is the
lack of notational distinction between a vector and a scalar. All vectors are
“column” vectors, although I usually write them as horizontal lists of their
elements. (Whether vectors are “row” vectors or “column” vectors is generally
only relevant for how we write expressions involving vector/matrix multipli-
cation or partitions of matrices.)

I write algorithms in various ways, sometimes in a form that looks similar
to Fortran or C and sometimes as a list of numbered steps. I believe all of the
descriptions used are straightforward and unambiguous.

This book could serve as a basic reference either for courses in statistical
computing or for courses in linear models or multivariate analysis. When the
book is used as a reference, rather than looking for “Deﬁnition” or “Theorem”,
the user should look for items set oﬀ with bullets or look for numbered equa-
tions, or else should use the Index, beginning on page 519, or Appendix A,
beginning on page 479.

The prerequisites for this text are minimal. Obviously some background in
mathematics is necessary. Some background in statistics or data analysis and
some level of scientiﬁc computer literacy are also required. References to rather
advanced mathematical topics are made in a number of places in the text. To
some extent this is because many sections evolved from class notes that I
developed for various courses that I have taught. All of these courses were at
the graduate level in the computational and statistical sciences, but they have
had wide ranges in mathematical level. I have carefully reread the sections
that refer to groups, ﬁelds, measure theory, and so on, and am convinced that
if the reader does not know much about these topics, the material is still
understandable, but if the reader is familiar with these topics, the references

xii

Preface

add to that reader’s appreciation of the material. In many places, I refer to
computer programming, and some of the exercises require some programming.
A careful coverage of Part III requires background in numerical programming.
In regard to the use of the book as a text, most of the book evolved in one
way or another for my own use in the classroom. I must quickly admit, how-
ever, that I have never used this whole book as a text for any single course. I
have used Part III in the form of printed notes as the primary text for a course
in the “foundations of computational science” taken by graduate students in
the natural sciences (including a few statistics students, but dominated by
physics students). I have provided several sections from Parts I and II in online
PDF ﬁles as supplementary material for a two-semester course in mathemati-
cal statistics at the “baby measure theory” level (using Shao, 2003). Likewise,
for my courses in computational statistics and statistical visualization, I have
provided many sections, either as supplementary material or as the primary
text, in online PDF ﬁles or printed notes. I have not taught a regular “applied
statistics” course in almost 30 years, but if I did, I am sure that I would draw
heavily from Parts I and II for courses in regression or multivariate analysis.
If I ever taught a course in “matrices for statistics” (I don’t even know if
such courses exist), this book would be my primary text because I think it
covers most of the things statisticians need to know about matrix theory and
computations.

Some exercises are Monte Carlo studies. I do not discuss Monte Carlo
methods in this text, so the reader lacking background in that area may need
to consult another reference in order to work those exercises. The exercises
should be considered an integral part of the book. For some exercises, the
required software can be obtained from either statlib or netlib (see the
bibliography). Exercises in any of the chapters, not just in Part III, may
require computations or computer programming.

Penultimately, I must make some statement about the relationship of
this book to some other books on similar topics. Much important statisti-
cal theory and many methods make use of matrix theory, and many sta-
tisticians have contributed to the advancement of matrix theory from its
very early days. Widely used books with derivatives of the words “statis-
tics” and “matrices/linear-algebra” in their titles include Basilevsky (1983),
Graybill (1983), Harville (1997), Schott (2004), and Searle (1982). All of these
are useful books. The computational orientation of this book is probably the
main diﬀerence between it and these other books. Also, some of these other
books only address topics of use in linear models, whereas this book also dis-
cusses matrices useful in graph theory, stochastic processes, and other areas
of application. (If the applications are only in linear models, most matrices
of interest are symmetric, and all eigenvalues can be considered to be real.)
Other diﬀerences among all of these books, of course, involve the authors’
choices of secondary topics and the ordering of the presentation.

Preface

xiii

Acknowledgments

I thank John Kimmel of Springer for his encouragement and advice on this
book and other books on which he has worked with me. I especially thank
Ken Berk for his extensive and insightful comments on a draft of this book.
I thank my student Li Li for reading through various drafts of some of the
chapters and pointing out typos or making helpful suggestions. I thank the
anonymous reviewers of this edition for their comments and suggestions. I also
thank the many readers of my previous book on numerical linear algebra who
informed me of errors and who otherwise provided comments or suggestions
for improving the exposition. Whatever strengths this book may have can be
attributed in large part to these people, named or otherwise. The weaknesses
can only be attributed to my own ignorance or hardheadedness.
I thank my wife, Mar´ıa, to whom this book is dedicated, for everything.

I used TEX via LATEX 2ε to write the book. I did all of the typing, program-
ming, etc., myself, so all misteaks are mine. I would appreciate receiving
suggestions for improvement and notiﬁcation of errors. Notes on this book,
including errata, are available at

http://mason.gmu.edu/~jgentle/books/matbk/

Fairfax County, Virginia

James E. Gentle
June 12, 2007

Contents

Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii

Part I Linear Algebra

1 Basic Vector/Matrix Structure and Notation . . . . . . . . . . . . . .
1.1 Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Arrays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4 Representation of Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3
4
5
5
7

2 Vectors and Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1 Operations on Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9
9
2.1.1 Linear Combinations and Linear Independence . . . . . . . . 10
2.1.2 Vector Spaces and Spaces of Vectors . . . . . . . . . . . . . . . . . 11
2.1.3 Basis Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.1.4 Inner Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.1.5 Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2.1.6 Normalized Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.1.7 Metrics and Distances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
2.1.8 Orthogonal Vectors and Orthogonal Vector Spaces . . . . . 22
2.1.9 The “One Vector” . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.2 Cartesian Coordinates and Geometrical Properties of Vectors . 24
2.2.1 Cartesian Geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
2.2.2 Projections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
2.2.3 Angles between Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.2.4 Orthogonalization Transformations . . . . . . . . . . . . . . . . . . 27
2.2.5 Orthonormal Basis Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
2.2.6 Approximation of Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . 30
2.2.7 Flats, Aﬃne Spaces, and Hyperplanes . . . . . . . . . . . . . . . . 31
2.2.8 Cones . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

xvi

Contents

2.2.9 Cross Products in IR3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
2.3 Centered Vectors and Variances and Covariances of Vectors . . . 33
2.3.1 The Mean and Centered Vectors . . . . . . . . . . . . . . . . . . . . 34
2.3.2 The Standard Deviation, the Variance,

and Scaled Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
2.3.3 Covariances and Correlations between Vectors . . . . . . . . 36
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

3 Basic Properties of Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.1 Basic Deﬁnitions and Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.1.1 Matrix Shaping Operators . . . . . . . . . . . . . . . . . . . . . . . . . . 44
3.1.2 Partitioned Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
3.1.3 Matrix Addition. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
3.1.4 Scalar-Valued Operators on Square Matrices:

The Trace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49

3.1.5 Scalar-Valued Operators on Square Matrices:

The Determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50

3.2 Multiplication of Matrices and Multiplication

of Vectors and Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
3.2.1 Matrix Multiplication (Cayley) . . . . . . . . . . . . . . . . . . . . . . 59
3.2.2 Multiplication of Partitioned Matrices. . . . . . . . . . . . . . . . 61
3.2.3 Elementary Operations on Matrices . . . . . . . . . . . . . . . . . . 61
3.2.4 Traces and Determinants of Square Cayley Products . . . 67
3.2.5 Multiplication of Matrices and Vectors . . . . . . . . . . . . . . . 68
3.2.6 Outer Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
3.2.7 Bilinear and Quadratic Forms; Deﬁniteness . . . . . . . . . . . 69
3.2.8 Anisometric Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
3.2.9 Other Kinds of Matrix Multiplication . . . . . . . . . . . . . . . . 72
3.3 Matrix Rank and the Inverse of a Full Rank Matrix . . . . . . . . . . 76

3.3.1 The Rank of Partitioned Matrices, Products

of Matrices, and Sums of Matrices . . . . . . . . . . . . . . . . . . . 78
3.3.2 Full Rank Partitioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
3.3.3 Full Rank Matrices and Matrix Inverses . . . . . . . . . . . . . . 81
3.3.4 Full Rank Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
3.3.5 Equivalent Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
3.3.6 Multiplication by Full Rank Matrices . . . . . . . . . . . . . . . . 88
3.3.7 Products of the Form ATA . . . . . . . . . . . . . . . . . . . . . . . . . 90
3.3.8 A Lower Bound on the Rank of a Matrix Product . . . . . 92
3.3.9 Determinants of Inverses . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
3.3.10 Inverses of Products and Sums of Matrices . . . . . . . . . . . 93
3.3.11 Inverses of Matrices with Special Forms . . . . . . . . . . . . . . 94
3.3.12 Determining the Rank of a Matrix . . . . . . . . . . . . . . . . . . . 94
95
3.4.1 Inverses of Partitioned Matrices . . . . . . . . . . . . . . . . . . . . . 95
3.4.2 Determinants of Partitioned Matrices . . . . . . . . . . . . . . . . 96

3.4 More on Partitioned Square Matrices: The Schur Complement

Contents

xvii

3.5 Linear Systems of Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
3.5.1 Solutions of Linear Systems . . . . . . . . . . . . . . . . . . . . . . . . . 97
3.5.2 Null Space: The Orthogonal Complement . . . . . . . . . . . . . 99
3.6 Generalized Inverses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
3.6.1 Generalized Inverses of Sums of Matrices . . . . . . . . . . . . . 101
3.6.2 Generalized Inverses of Partitioned Matrices . . . . . . . . . . 101
3.6.3 Pseudoinverse or Moore-Penrose Inverse . . . . . . . . . . . . . . 101
3.7 Orthogonality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
3.8 Eigenanalysis; Canonical Factorizations . . . . . . . . . . . . . . . . . . . . 105
3.8.1 Basic Properties of Eigenvalues and Eigenvectors . . . . . . 107
3.8.2 The Characteristic Polynomial . . . . . . . . . . . . . . . . . . . . . . 108
3.8.3 The Spectrum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
3.8.4 Similarity Transformations . . . . . . . . . . . . . . . . . . . . . . . . . 114
3.8.5 Similar Canonical Factorization;

Diagonalizable Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
3.8.6 Properties of Diagonalizable Matrices . . . . . . . . . . . . . . . . 118
3.8.7 Eigenanalysis of Symmetric Matrices . . . . . . . . . . . . . . . . . 119
3.8.8 Positive Deﬁnite and Nonnegative Deﬁnite Matrices . . . 124
3.8.9 The Generalized Eigenvalue Problem . . . . . . . . . . . . . . . . 126
3.8.10 Singular Values and the Singular Value Decomposition . 127
3.9 Matrix Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
3.9.1 Matrix Norms Induced from Vector Norms . . . . . . . . . . . 129
3.9.2 The Frobenius Norm — The “Usual” Norm . . . . . . . . . . . 131
3.9.3 Matrix Norm Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
3.9.4 The Spectral Radius . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
3.9.5 Convergence of a Matrix Power Series . . . . . . . . . . . . . . . . 134
3.10 Approximation of Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140

4 Vector/Matrix Derivatives and Integrals . . . . . . . . . . . . . . . . . . . 145
4.1 Basics of Diﬀerentiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
4.2 Types of Diﬀerentiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
4.2.1 Diﬀerentiation with Respect to a Scalar . . . . . . . . . . . . . . 149
4.2.2 Diﬀerentiation with Respect to a Vector . . . . . . . . . . . . . . 150
4.2.3 Diﬀerentiation with Respect to a Matrix . . . . . . . . . . . . . 154
4.3 Optimization of Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
4.3.1 Stationary Points of Functions . . . . . . . . . . . . . . . . . . . . . . 156
4.3.2 Newton’s Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
4.3.3 Optimization of Functions with Restrictions . . . . . . . . . . 159
4.4 Multiparameter Likelihood Functions . . . . . . . . . . . . . . . . . . . . . . 163
4.5 Integration and Expectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164

4.5.1 Multidimensional Integrals and Integrals Involving

Vectors and Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
4.5.2 Integration Combined with Other Operations . . . . . . . . . 166
4.5.3 Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167

xviii

Contents

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169

5 Matrix Transformations and Factorizations . . . . . . . . . . . . . . . . 173
5.1 Transformations by Orthogonal Matrices . . . . . . . . . . . . . . . . . . . 174
5.2 Geometric Transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
5.2.1 Rotations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
5.2.2 Reﬂections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
5.2.3 Translations; Homogeneous Coordinates . . . . . . . . . . . . . . 178
5.3 Householder Transformations (Reﬂections) . . . . . . . . . . . . . . . . . . 180
5.4 Givens Transformations (Rotations) . . . . . . . . . . . . . . . . . . . . . . . 182
5.5 Factorization of Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
5.6 LU and LDU Factorizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
5.7 QR Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
5.7.1 Householder Reﬂections to Form the QR Factorization . 190
5.7.2 Givens Rotations to Form the QR Factorization . . . . . . . 192
5.7.3 Gram-Schmidt Transformations to Form the

QR Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
5.8 Singular Value Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
5.9 Factorizations of Nonnegative Deﬁnite Matrices . . . . . . . . . . . . . 193
5.9.1 Square Roots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
5.9.2 Cholesky Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
5.9.3 Factorizations of a Gramian Matrix . . . . . . . . . . . . . . . . . . 196
5.10 Incomplete Factorizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198

6

Solution of Linear Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
6.1 Condition of Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
6.2 Direct Methods for Consistent Systems . . . . . . . . . . . . . . . . . . . . . 206
6.2.1 Gaussian Elimination and Matrix Factorizations . . . . . . . 207
6.2.2 Choice of Direct Method . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
6.3 Iterative Methods for Consistent Systems . . . . . . . . . . . . . . . . . . . 211

6.3.1 The Gauss-Seidel Method with

Successive Overrelaxation . . . . . . . . . . . . . . . . . . . . . . . . . . 212

6.3.2 Conjugate Gradient Methods for Symmetric

Positive Deﬁnite Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
6.3.3 Multigrid Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
6.4 Numerical Accuracy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
6.5 Iterative Reﬁnement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
6.6 Updating a Solution to a Consistent System . . . . . . . . . . . . . . . . 220
6.7 Overdetermined Systems; Least Squares . . . . . . . . . . . . . . . . . . . . 222
6.7.1 Least Squares Solution of an Overdetermined System . . 224
6.7.2 Least Squares with a Full Rank Coeﬃcient Matrix . . . . . 226
6.7.3 Least Squares with a Coeﬃcient Matrix

Not of Full Rank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227

Contents

xix

6.7.4 Updating a Least Squares Solution

of an Overdetermined System . . . . . . . . . . . . . . . . . . . . . . . 228
6.8 Other Solutions of Overdetermined Systems. . . . . . . . . . . . . . . . . 229
6.8.1 Solutions that Minimize Other Norms of the Residuals . 230
6.8.2 Regularized Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
6.8.3 Minimizing Orthogonal Distances. . . . . . . . . . . . . . . . . . . . 234
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238

7 Evaluation of Eigenvalues and Eigenvectors . . . . . . . . . . . . . . . . 241
7.1 General Computational Methods . . . . . . . . . . . . . . . . . . . . . . . . . . 242
7.1.1 Eigenvalues from Eigenvectors and Vice Versa. . . . . . . . . 242
7.1.2 Deﬂation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
7.1.3 Preconditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244
7.2 Power Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
7.3 Jacobi Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
7.4 QR Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250
7.5 Krylov Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252
7.6 Generalized Eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252
7.7 Singular Value Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256

Part II Applications in Data Analysis

8

Special Matrices and Operations Useful in Modeling
and Data Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
8.1 Data Matrices and Association Matrices . . . . . . . . . . . . . . . . . . . . 261
8.1.1 Flat Files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
8.1.2 Graphs and Other Data Structures . . . . . . . . . . . . . . . . . . 262
8.1.3 Probability Distribution Models . . . . . . . . . . . . . . . . . . . . . 269
8.1.4 Association Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
8.2 Symmetric Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
8.3 Nonnegative Deﬁnite Matrices; Cholesky Factorization . . . . . . . 275
8.4 Positive Deﬁnite Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
8.5 Idempotent and Projection Matrices . . . . . . . . . . . . . . . . . . . . . . . 280
8.5.1 Idempotent Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281
8.5.2 Projection Matrices: Symmetric Idempotent Matrices . . 286
8.6 Special Matrices Occurring in Data Analysis . . . . . . . . . . . . . . . . 287
8.6.1 Gramian Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288
8.6.2 Projection and Smoothing Matrices . . . . . . . . . . . . . . . . . . 290
8.6.3 Centered Matrices and Variance-Covariance Matrices . . 293
8.6.4 The Generalized Variance . . . . . . . . . . . . . . . . . . . . . . . . . . 296
8.6.5 Similarity Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298
8.6.6 Dissimilarity Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
8.7 Nonnegative and Positive Matrices . . . . . . . . . . . . . . . . . . . . . . . . . 299

xx

Contents

9

8.7.1 Properties of Square Positive Matrices . . . . . . . . . . . . . . . 301
8.7.2 Irreducible Square Nonnegative Matrices . . . . . . . . . . . . . 302
8.7.3 Stochastic Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
8.7.4 Leslie Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
8.8 Other Matrices with Special Structures . . . . . . . . . . . . . . . . . . . . . 307
8.8.1 Helmert Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308
8.8.2 Vandermonde Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309
8.8.3 Hadamard Matrices and Orthogonal Arrays . . . . . . . . . . . 310
8.8.4 Toeplitz Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311
8.8.5 Hankel Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312
8.8.6 Cauchy Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313
8.8.7 Matrices Useful in Graph Theory . . . . . . . . . . . . . . . . . . . . 313
8.8.8 M-Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317

Selected Applications in Statistics . . . . . . . . . . . . . . . . . . . . . . . . . 321
9.1 Multivariate Probability Distributions . . . . . . . . . . . . . . . . . . . . . . 322
9.1.1 Basic Deﬁnitions and Properties . . . . . . . . . . . . . . . . . . . . . 322
9.1.2 The Multivariate Normal Distribution. . . . . . . . . . . . . . . . 323
9.1.3 Derived Distributions and Cochran’s Theorem . . . . . . . . 323
9.2 Linear Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325
9.2.1 Fitting the Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
9.2.2 Linear Models and Least Squares . . . . . . . . . . . . . . . . . . . . 330
9.2.3 Statistical Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332
9.2.4 The Normal Equations and the Sweep Operator . . . . . . . 335
9.2.5 Linear Least Squares Subject to Linear

Equality Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337
9.2.6 Weighted Least Squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337
9.2.7 Updating Linear Regression Statistics . . . . . . . . . . . . . . . . 338
9.2.8 Linear Smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
9.3 Principal Components. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
9.3.1 Principal Components of a Random Vector . . . . . . . . . . . 342
9.3.2 Principal Components of Data . . . . . . . . . . . . . . . . . . . . . . 343
9.4 Condition of Models and Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346
9.4.1 Ill-Conditioning in Statistical Applications . . . . . . . . . . . . 346
9.4.2 Variable Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347
9.4.3 Principal Components Regression . . . . . . . . . . . . . . . . . . . 348
9.4.4 Shrinkage Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348
9.4.5 Testing the Rank of a Matrix . . . . . . . . . . . . . . . . . . . . . . . 350
9.4.6 Incomplete Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352
9.5 Optimal Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355
9.6 Multivariate Random Number Generation . . . . . . . . . . . . . . . . . . 358
9.7 Stochastic Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360
9.7.1 Markov Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360
9.7.2 Markovian Population Models. . . . . . . . . . . . . . . . . . . . . . . 362

9.7.3 Autoregressive Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . 364
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365

Contents

xxi

Part III Numerical Methods and Software

10 Numerical Methods. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375
10.1 Digital Representation of Numeric Data . . . . . . . . . . . . . . . . . . . . 377
10.1.1 The Fixed-Point Number System . . . . . . . . . . . . . . . . . . . . 378
10.1.2 The Floating-Point Model for Real Numbers . . . . . . . . . . 379
10.1.3 Language Constructs for Representing Numeric Data . . 386
10.1.4 Other Variations in the Representation of Data;

Portability of Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391
10.2 Computer Operations on Numeric Data . . . . . . . . . . . . . . . . . . . . 393
10.2.1 Fixed-Point Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394
10.2.2 Floating-Point Operations . . . . . . . . . . . . . . . . . . . . . . . . . . 395
10.2.3 Exact Computations; Rational Fractions . . . . . . . . . . . . . 399
10.2.4 Language Constructs for Operations

on Numeric Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
10.3 Numerical Algorithms and Analysis . . . . . . . . . . . . . . . . . . . . . . . . 403
10.3.1 Error in Numerical Computations . . . . . . . . . . . . . . . . . . . 404
10.3.2 Eﬃciency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 412
10.3.3 Iterations and Convergence . . . . . . . . . . . . . . . . . . . . . . . . . 417
10.3.4 Other Computational Techniques . . . . . . . . . . . . . . . . . . . . 419
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422

11 Numerical Linear Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429
11.1 Computer Representation of Vectors and Matrices . . . . . . . . . . . 429
11.2 General Computational Considerations

for Vectors and Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431
11.2.1 Relative Magnitudes of Operands . . . . . . . . . . . . . . . . . . . . 431
11.2.2 Iterative Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433
11.2.3 Assessing Computational Errors . . . . . . . . . . . . . . . . . . . . . 434
11.3 Multiplication of Vectors and Matrices . . . . . . . . . . . . . . . . . . . . . 435
11.4 Other Matrix Computations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 439
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 441

12 Software for Numerical Linear Algebra . . . . . . . . . . . . . . . . . . . . 445
12.1 Fortran and C . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447
12.1.1 Programming Considerations . . . . . . . . . . . . . . . . . . . . . . . 448
12.1.2 Fortran 95 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452
12.1.3 Matrix and Vector Classes in C++ . . . . . . . . . . . . . . . . . . 453
12.1.4 Libraries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454
12.1.5 The IMSLTM Libraries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457
12.1.6 Libraries for Parallel Processing . . . . . . . . . . . . . . . . . . . . . 460

xxii

Contents

12.2 Interactive Systems for Array Manipulation . . . . . . . . . . . . . . . . . 461
12.2.1 MATLAB R⃝ and Octave . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463
12.2.2 R and S-PLUS R⃝ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 466
12.3 High-Performance Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470
12.4 Software for Statistical Applications . . . . . . . . . . . . . . . . . . . . . . . 472
12.5 Test Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 472
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 475

A Notation and Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 479
A.1 General Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 479
A.2 Computer Number Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481
A.3 General Mathematical Functions and Operators . . . . . . . . . . . . . 482
A.4 Linear Spaces and Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 484
A.5 Models and Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 490

B Solutions and Hints for Selected Exercises . . . . . . . . . . . . . . . . . 493

Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 505

Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 519

1

Basic Vector/Matrix Structure and Notation

Vectors and matrices are useful in representing multivariate data, and they
occur naturally in working with linear equations or when expressing linear
relationships among objects. Numerical algorithms for a variety of tasks in-
volve matrix and vector arithmetic. An optimization algorithm to ﬁnd the
minimum of a function, for example, may use a vector of ﬁrst derivatives and
a matrix of second derivatives; and a method to solve a diﬀerential equation
may use a matrix with a few diagonals for computing diﬀerences.

There are various precise ways of deﬁning vectors and matrices, but we
will generally think of them merely as linear or rectangular arrays of numbers,
or scalars, on which an algebra is deﬁned. Unless otherwise stated, we will as-
sume the scalars are real numbers. We denote both the set of real numbers
and the ﬁeld of real numbers as IR. (The ﬁeld is the set together with the op-
erators.) Occasionally we will take a geometrical perspective for vectors and
will consider matrices to deﬁne geometrical transformations. In all contexts,
however, the elements of vectors or matrices are real numbers (or, more gen-
erally, members of a ﬁeld). When this is not the case, we will use more general
phrases, such as “ordered lists” or “arrays”.

Many of the operations covered in the ﬁrst few chapters, especially the
transformations and factorizations in Chapter 5, are important because of
their use in solving systems of linear equations, which will be discussed in
Chapter 6; in computing eigenvectors, eigenvalues, and singular values, which
will be discussed in Chapter 7; and in the applications in Chapter 9.

Throughout the ﬁrst few chapters, we emphasize the facts that are impor-
tant in statistical applications. We also occasionally refer to relevant compu-
tational issues, although computational details are addressed speciﬁcally in
Part III.

It is very important to understand that the form of a mathematical expres-
sion and the way the expression should be evaluated in actual practice may
be quite diﬀerent. We remind the reader of this fact from time to time. That
there is a diﬀerence in mathematical expressions and computational methods
is one of the main messages of Chapters 10 and 11. (An example of this, in

4

1 Basic Vector/Matrix Notation

notation that we will introduce later, is the expression A−1b. If our goal is to
solve a linear system Ax = b, we probably should never compute the matrix
inverse A−1 and then multiply it times b. Nevertheless, it may be entirely
appropriate to write the expression A−1b.)

1.1 Vectors

For a positive integer n, a vector (or n-vector) is an n-tuple, ordered (multi)set,
or array of n numbers, called elements or scalars. The number of elements
is called the order, or sometimes the “length”, of the vector. An n-vector
can be thought of as representing a point in n-dimensional space. In this
setting, the “length” of the vector may also mean the Euclidean distance from
the origin to the point represented by the vector; that is, the square root of
the sum of the squares of the elements of the vector. This Euclidean distance
will generally be what we mean when we refer to the length of a vector (see
page 17).

We usually use a lowercase letter to represent a vector, and we use the

same letter with a single subscript to represent an element of the vector.

The ﬁrst element of an n-vector is the ﬁrst (1st) element and the last is the
nth element. (This statement is not a tautology; in some computer systems,
the ﬁrst element of an object used to represent a vector is the 0th element
of the object. This sometimes makes it diﬃcult to preserve the relationship
between the computer entity and the object that is of interest.) We will use
paradigms and notation that maintain the priority of the object of interest
rather than the computer entity representing it.

We may write the n-vector x as

x =⎛⎜⎝

x1
...
xn

⎞⎟⎠

(1.1)

or

x = (x1, . . . , xn).

(1.2)
We make no distinction between these two notations, although in some con-
texts we think of a vector as a “column”, so the ﬁrst notation may be more
natural. The simplicity of the second notation recommends it for common use.
(And this notation does not require the additional symbol for transposition
that some people use when they write the elements of a vector horizontally.)

We use the notation

IRn

to denote the set of n-vectors with real elements.

1.3 Matrices

5

1.2 Arrays

Arrays are structured collections of elements corresponding in shape to lines,
rectangles, or rectangular solids. The number of dimensions of an array is often
called the rank of the array. Thus, a vector is an array of rank 1, and a matrix
is an array of rank 2. A scalar, which can be thought of as a degenerate array,
has rank 0. When referring to computer software objects, “rank” is generally
used in this sense. (This term comes from its use in describing a tensor. A
rank 0 tensor is a scalar, a rank 1 tensor is a vector, a rank 2 tensor is a
square matrix, and so on. In our usage referring to arrays, we do not require
that the dimensions be equal, however.) When we refer to “rank of an array”,
we mean the number of dimensions. When we refer to “rank of a matrix”, we
mean something diﬀerent, as we discuss in Section 3.3. In linear algebra, this
latter usage is far more common than the former.

1.3 Matrices

A matrix is a rectangular or two-dimensional array. We speak of the rows and
columns of a matrix. The rows or columns can be considered to be vectors,
and we often use this equivalence. An n × m matrix is one with n rows and
m columns. The number of rows and the number of columns determine the
shape of the matrix. Note that the shape is the doubleton (n, m), not just
a single number such as the ratio. If the number of rows is the same as the
number of columns, the matrix is said to be square.

All matrices are two-dimensional in the sense of “dimension” used above.
The word “dimension”, however, when applied to matrices, often means some-
thing diﬀerent, namely the number of columns. (This usage of “dimension” is
common both in geometry and in traditional statistical applications.)

We usually use an uppercase letter to represent a matrix. To represent an
element of the matrix, we usually use the corresponding lowercase letter with
a subscript to denote the row and a second subscript to represent the column.
If a nontrivial expression is used to denote the row or the column, we separate
the row and column subscripts with a comma.

Although vectors and matrices are fundamentally quite diﬀerent types of
objects, we can bring some unity to our discussion and notation by occasion-
ally considering a vector to be a “column vector” and in some ways to be the
same as an n × 1 matrix. (This has nothing to do with the way we may write
the elements of a vector. The notation in equation (1.2) is more convenient
than that in equation (1.1) and so will generally be used in this book, but its
use should not change the nature of the vector. Likewise, this has nothing to
do with the way the elements of a vector or a matrix are stored in the com-
puter.) When we use vectors and matrices in the same expression, however,
we use the symbol “T” (for “transpose”) as a superscript to represent a vector
that is being treated as a 1 × n matrix.

6

1 Basic Vector/Matrix Notation

We use the notation a∗j to correspond to the jth column of the matrix A
and use ai∗ to represent the (column) vector that corresponds to the ith row.
The ﬁrst row is the 1st (ﬁrst) row, and the ﬁrst column is the 1st (ﬁrst)
column. (Again, we remark that computer entities used in some systems to
represent matrices and to store elements of matrices as computer data some-
times index the elements beginning with 0. Furthermore, some systems use the
ﬁrst index to represent the column and the second index to indicate the row.
We are not speaking here of the storage order — “row major” versus “column
major” — we address that later, in Chapter 11. Rather, we are speaking of the
mechanism of referring to the abstract entities. In image processing, for exam-
ple, it is common practice to use the ﬁrst index to represent the column and
the second index to represent the row. In the software package PV-Wave, for
example, there are two diﬀerent kinds of two-dimensional objects: “arrays”, in
which the indexing is done as in image processing, and “matrices”, in which
the indexing is done as we have described.)

The n × m matrix A can be written

A =⎡⎢⎣

a11 . . . a1m
...
...
an1 . . . anm

...

⎤⎥⎦ .

(1.3)

We also write the matrix A above as

A = (aij),

(1.4)
with the indices i and j ranging over {1, . . . , n} and {1, . . . , m}, respectively.
We use the notation An×m to refer to the matrix A and simultaneously to
indicate that it is n × m, and we use the notation

IRn×m

to refer to the set of all n × m matrices with real elements.
jth column of the matrix A; that is, in equation (1.3), (A)ij = aij.

We use the notation (A)ij to refer to the element in the ith row and the

Although vectors are column vectors and the notation in equations (1.1)
and (1.2) represents the same entity, that would not be the same for matrices.
If x1, . . . , xn are scalars

(1.5)

X =⎡⎢⎣

x1
...
xn

⎤⎥⎦

and

(1.6)
then X is an n × 1 matrix and Y is a 1 × n matrix (and Y is the transpose
of X). Although an n × 1 matrix is a diﬀerent type of object from a vector,

Y = [x1, . . . , xn],

1.4 Representation of Data

7

we may treat X in equation (1.5) or Y T in equation (1.6) as a vector when
it is convenient to do so. Furthermore, although a 1 × 1 matrix, a 1-vector,
and a scalar are all fundamentally diﬀerent types of objects, we will treat a
one by one matrix or a vector with only one element as a scalar whenever it
is convenient.

One of the most important uses of matrices is as a transformation of a vec-
tor by vector/matrix multiplication. Such transformations are linear (a term
that we deﬁne later). Although one can occasionally proﬁtably distinguish ma-
trices from linear transformations on vectors, for our present purposes there
is no advantage in doing so. We will often treat matrices and linear transfor-
mations as equivalent.

Many of the properties of vectors and matrices we discuss hold for an
inﬁnite number of elements, but we will assume throughout this book that
the number is ﬁnite.

Subvectors and Submatrices

We sometimes ﬁnd it useful to work with only some of the elements of a
vector or matrix. We refer to the respective arrays as “subvectors” or “sub-
matrices”. We also allow the rearrangement of the elements by row or column
permutations and still consider the resulting object as a subvector or subma-
trix. In Chapter 3, we will consider special forms of submatrices formed by
“partitions” of given matrices.

1.4 Representation of Data

Before we can do any serious analysis of data, the data must be represented
in some structure that is amenable to the operations of the analysis. In sim-
ple cases, the data are represented by a list of scalar values. The ordering in
the list may be unimportant, and the analysis may just consist of computa-
tion of simple summary statistics. In other cases, the list represents a time
series of observations, and the relationships of observations to each other as
a function of their distance apart in the list are of interest. Often, the data
can be represented meaningfully in two lists that are related to each other by
the positions in the lists. The generalization of this representation is a two-
dimensional array in which each column corresponds to a particular type of
data.

A major consideration, of course, is the nature of the individual items of
data. The observational data may be in various forms: quantitative measures,
colors, text strings, and so on. Prior to most analyses of data, they must be
represented as real numbers. In some cases, they can be represented easily
as real numbers, although there may be restrictions on the mapping into the
reals. (For example, do the data naturally assume only integral values, or
could any real number be mapped back to a possible observation?)

8

1 Basic Vector/Matrix Notation

The most common way of representing data is by using a two-dimensional
array in which the rows correspond to observational units (“instances”) and
the columns correspond to particular types of observations (“variables” or
“features”). If the data correspond to real numbers, this representation is the
familiar X data matrix. Much of this book is devoted to the matrix theory
and computational methods for the analysis of data in this form. This type of
matrix, perhaps with an adjoined vector, is the basic structure used in many
familiar statistical methods, such as regression analysis, principal components
analysis, analysis of variance, multidimensional scaling, and so on.

There are other types of structures that are useful in representing data
based on graphs. A graph is a structure consisting of two components: a set of
points, called vertices or nodes and a set of pairs of the points, called edges.
(Note that this usage of the word “graph” is distinctly diﬀerent from the
more common one that refers to lines, curves, bars, and so on to represent
data pictorially. The phrase “graph theory” is often used, or overused, to em-
phasize the present meaning of the word.) A graph G = (V, E) with vertices
V = {v1, . . . , vn} is distinguished primarily by the nature of the edge elements
(vi, vj) in E. Graphs are identiﬁed as complete graphs, directed graphs, trees,
and so on, depending on E and its relationship with V . A tree may be used
for data that are naturally aggregated in a hierarchy, such as political unit,
subunit, household, and individual. Trees are also useful for representing clus-
tering of data at diﬀerent levels of association. In this type of representation,
the individual data elements are the leaves of the tree.

In another type of graphical representation that is often useful in “data
mining”, where we seek to uncover relationships among objects, the vertices
are the objects, either observational units or features, and the edges indicate
some commonality between vertices. For example, the vertices may be text
documents, and an edge between two documents may indicate that a certain
number of speciﬁc words or phrases occur in both documents. Despite the
diﬀerences in the basic ways of representing data, in graphical modeling of
data, many of the standard matrix operations used in more traditional data
analysis are applied to matrices that arise naturally from the graph.

However the data are represented, whether in an array or a network, the
analysis of the data is often facilitated by using “association” matrices. The
most familiar type of association matrix is perhaps a correlation matrix. We
will encounter and use other types of association matrices in Chapter 8.

2

Vectors and Vector Spaces

In this chapter we discuss a wide range of basic topics related to vectors of real
numbers. Some of the properties carry over to vectors over other ﬁelds, such
as complex numbers, but the reader should not assume this. Occasionally, for
emphasis, we will refer to “real” vectors or “real” vector spaces, but unless it
is stated otherwise, we are assuming the vectors and vector spaces are real.
The topics and the properties of vectors and vector spaces that we emphasize
are motivated by applications in the data sciences.

2.1 Operations on Vectors

The elements of the vectors we will use in the following are real numbers, that
is, elements of IR. We call elements of IR scalars. Vector operations are deﬁned
in terms of operations on real numbers.

Two vectors can be added if they have the same number of elements.
The sum of two vectors is the vector whose elements are the sums of the
corresponding elements of the vectors being added. Vectors with the same
number of elements are said to be conformable for addition. A vector all of
whose elements are 0 is the additive identity for all conformable vectors.

We overload the usual symbols for the operations on the reals to signify
the corresponding operations on vectors or matrices when the operations are
deﬁned. Hence, “+” can mean addition of scalars, addition of conformable
vectors, or addition of a scalar to a vector. This last meaning of “+” may
not be used in many mathematical treatments of vectors, but it is consistent
with the semantics of modern computer languages such as Fortran 95, R, and
Matlab. By the addition of a scalar to a vector, we mean the addition of the
scalar to each element of the vector, resulting in a vector of the same number
of elements.

A scalar multiple of a vector (that is, the product of a real number and
a vector) is the vector whose elements are the multiples of the corresponding
elements of the original vector. Juxtaposition of a symbol for a scalar and a

10

2 Vectors and Vector Spaces

symbol for a vector indicates the multiplication of the scalar with each element
of the vector, resulting in a vector of the same number of elements.

A very common operation in working with vectors is the addition of a

scalar multiple of one vector to another vector,

z = ax + y,

(2.1)

where a is a scalar and x and y are vectors conformable for addition. Viewed
as a single operation with three operands, this is called an “axpy” for obvious
reasons. (Because the Fortran versions of BLAS to perform this operation
were called saxpy and daxpy, the operation is also sometimes called “saxpy”
or “daxpy”. See Section 12.1.4 on page 454, for a description of the BLAS.)
The axpy operation is called a linear combination. Such linear combinations
of vectors are the basic operations in most areas of linear algebra. The com-
position of axpy operations is also an axpy; that is, one linear combination
followed by another linear combination is a linear combination. Furthermore,
any linear combination can be decomposed into a sequence of axpy operations.

2.1.1 Linear Combinations and Linear Independence

If a given vector can be formed by a linear combination of one or more vectors,
the set of vectors (including the given one) is said to be linearly dependent;
conversely, if in a set of vectors no one vector can be represented as a linear
combination of any of the others, the set of vectors is said to be linearly
independent. In equation (2.1), for example, the vectors x, y, and z are not
linearly independent. It is possible, however, that any two of these vectors
are linearly independent. Linear independence is one of the most important
concepts in linear algebra.

We can see that the deﬁnition of a linearly independent set of vectors

{v1, . . . , vk} is equivalent to stating that if

a1v1 + ··· akvk = 0,

(2.2)
then a1 = ··· = ak = 0. If the set of vectors {v1, . . . , vk} is not linearly inde-
pendent, then it is possible to select a maximal linearly independent subset;
that is, a subset of {v1, . . . , vk} that is linearly independent and has maxi-
mum cardinality. We do this by selecting an arbitrary vector, vi1, and then
seeking a vector that is independent of vi1. If there are none in the set that
is linearly independent of vi1, then a maximum linearly independent subset
is just the singleton, because all of the vectors must be a linear combination
of just one vector (that is, a scalar multiple of that one vector). If there is a
vector that is linearly independent of vi1, say vi2, we next seek a vector in the
remaining set that is independent of vi1 and vi2. If one does not exist, then
{vi1, vi2} is a maximal subset because any other vector can be represented in
terms of these two and hence, within any subset of three vectors, one can be
represented in terms of the two others. Thus, we see how to form a maximal

2.1 Operations on Vectors

11

linearly independent subset, and we see that the maximum cardinality of any
subset of linearly independent vectors is unique.

It is easy to see that the maximum number of n-vectors that can form a
set that is linearly independent is n. (We can see this by assuming n linearly
independent vectors and then, for any (n + 1)th vector, showing that it is
a linear combination of the others by building it up one by one from linear
combinations of two of the given linearly independent vectors. In Exercise 2.1,
you are asked to write out these steps.)

Properties of a set of vectors are usually invariant to a permutation of the
elements of the vectors if the same permutation is applied to all vectors in the
set. In particular, if a set of vectors is linearly independent, the set remains
linearly independent if the elements of each vector are permuted in the same
way.

If the elements of each vector in a set of vectors are separated into sub-
vectors, linear independence of any set of corresponding subvectors implies
linear independence of the full vectors. To state this more precisely for a set
of three n-vectors, let x = (x1, . . . , xn), y = (y1, . . . , yn), and z = (z1, . . . , zn).
Now let {i1, . . . , ik} ⊂ {1, . . . , n}, and form the k-vectors ˜x = (xi1, . . . , xik),
˜y = (yi1, . . . , yik), and ˜z = (zi1, . . . , zik). Then linear independence of ˜x, ˜y,
and ˜z implies linear independence of x, y, and z.

2.1.2 Vector Spaces and Spaces of Vectors

Let V be a set of n-vectors such that any linear combination of the vectors in
V is also in V . Then the set together with the usual vector algebra is called a
vector space. (Technically, the “usual algebra” is a linear algebra consisting of
two operations: vector addition and scalar times vector multiplication, which
are the two operations comprising an axpy. It has closure of the space under
the combination of those operations, commutativity and associativity of addi-
tion, an additive identity and inverses, a multiplicative identity, distribution of
multiplication over both vector addition and scalar addition, and associativity
of scalar multiplication and scalar times vector multiplication. Vector spaces
are linear spaces.) A vector space necessarily includes the additive identity.
(In the axpy operation, let a = −1 and y = x.)
The key characteristic of a vector space is a linear algebra.

A vector space can also be made up of other objects, such as matrices.

We generally use a calligraphic font to denote a vector space; V, for exam-
ple. Often, however, we think of the vector space merely in terms of the set
of vectors on which it is built and denote it by an ordinary capital letter; V ,
for example.

The Order and the Dimension of a Vector Space

The maximum number of linearly independent vectors in a vector space is
called the dimension of the vector space. We denote the dimension by

12

2 Vectors and Vector Spaces

dim(·),

which is a mapping IRn #→ ZZ+ (where ZZ+ denotes the positive integers). The
length or order of the vectors in the space is the order of the vector space. The
order is greater than or equal to the dimension, as we showed above.

The vector space consisting of all n-vectors with real elements is denoted
IRn. (As mentioned earlier, the notation IRn also refers to just the set of
n-vectors with real elements; that is, to the set over which the vector space is
deﬁned.) Both the order and the dimension of IRn are n.

We also use the phrase dimension of a vector to mean the dimension of
the vector space of which the vector is an element. This term is ambiguous,
but its meaning is clear in certain applications, such as dimension reduction,
that we will discuss later.

Many of the properties of vectors that we discuss hold for an inﬁnite
number of elements, but throughout this book we will assume the vector
spaces have a ﬁnite number of dimensions.

Essentially Disjoint Vector Spaces
If the only element in common between two vector spaces V1 and V2 is the
additive identity, the spaces are said to be essentially disjoint. If the vector
spaces V1 and V2 are essentially disjoint, it is clear that any element in V1
(except the additive identity) is linearly independent of any set of elements
in V2.
Some Special Vectors

We denote the additive identity in a vector space of order n by 0n or sometimes
by 0. This is the vector consisting of all zeros. We call this the zero vector.
This vector by itself is sometimes called the null vector space. It is not a vector
space in the usual sense; it would have dimension 0. (All linear combinations
are the same.)

Likewise, we denote the vector consisting of all ones by 1n or sometimes by
1. We call this the one vector and also the “summing vector” (see page 23).
This vector and all scalar multiples of it are vector spaces with dimension
1. (This is true of any single nonzero vector; all linear combinations are just
scalar multiples.) Whether 0 and 1 without a subscript represent vectors or
scalars is usually clear from the context.

The ith unit vector, denoted by ei, has a 1 in the ith position and 0s in all

other positions:

(2.3)
Another useful vector is the sign vector, which is formed from signs of the

ei = (0, . . . , 0, 1, 0, . . . , 0).

elements of a given vector. It is denoted by “sign(·)” and deﬁned by

2.1 Operations on Vectors

13

sign(x)i = 1 if xi > 0,
= 0 if xi = 0,
= −1 if xi < 0.

(2.4)

Ordinal Relations among Vectors

There are several possible ways to form a rank ordering of vectors of the same
order, but no complete ordering is entirely satisfactory. (Note the unfortunate
overloading of the word “order” or “ordering” here.) If x and y are vectors of
the same order and for corresponding elements xi > yi, we say x is greater
than y and write

x > y.

(2.5)

In particular, if all of the elements of x are positive, we write x > 0.

If x and y are vectors of the same order and for corresponding elements

xi ≥ yi, we say x is greater than or equal to y and write

(2.6)
This relationship is a partial ordering (see Exercise 8.1a). The expression x ≥ 0
means that all of the elements of x are nonnegative.

x ≥ y.

Set Operations on Vector Spaces

Although a vector space is a set together with operations, we often speak of a
vector space as if it were just the set, and we use some of the same notation to
refer to vector spaces as we use to refer to sets. For example, if V is a vector
space, the notation W ⊆ V indicates that W is a vector space (that is, it has
the properties listed above), that the set of vectors in the vector space W is
a subset of the vectors in V, and that the operations in the two objects are
the same. A subset of a vector space V that is itself a vector space is called a
subspace of V.
The intersection of two vector spaces of the same order is a vector space.
The union of two such vector spaces, however, is not necessarily a vector space
(because for v1 ∈ V1 and v2 ∈ V2, v1 + v2 may not be in V1∪V2). We refer to a
set of vectors of the same order together with the addition operator (whether
or not the set is closed with respect to the operator) as a “space of vectors”.

If V1 and V2 are spaces of vectors, the space of vectors
V = {v, s.t. v = v1 + v2, v1 ∈ V1, v2 ∈ V2}

is called the sum of the spaces V1 and V2 and is denoted by V = V1 + V2. If
the spaces V1 and V2 are vector spaces, then V1 + V2 is a vector space, as is
easily veriﬁed.
If V1 and V2 are essentially disjoint vector spaces (not just spaces of vec-

tors), the sum is called the direct sum. This relation is denoted by

V = V1 ⊕ V2.

(2.7)

14

2 Vectors and Vector Spaces

Cones

A set of vectors that contains all positive scalar multiples of any vector in
the set is called a cone. A cone always contains the zero vector. A set of
vectors V is a convex cone if, for all v1, v2 ∈ V and all a, b ≥ 0, av1 + bv2 ∈ V .
(Such a cone is called a homogeneous convex cone by some authors. Also,
some authors require that a + b = 1 in the deﬁnition.) A convex cone is not
necessarily a vector space because v1 − v2 may not be in V . An important
convex cone in an n-dimensional vector space is the positive orthant together
with the zero vector. This convex cone is not closed, in the sense that it
does not contain some limits. The closure of the positive orthant (that is, the
nonnegative orthant) is also a convex cone.

2.1.3 Basis Sets
If each vector in the vector space V can be expressed as a linear combination
of the vectors in some set G, then G is said to be a generating set or spanning
set of V. If, in addition, all linear combinations of the elements of G are in
V, the vector space is the space generated by G and is denoted by V(G) or by
span(G):

V(G) ≡ span(G).

A set of linearly independent vectors that generate or span a space is said

to be a basis for the space.
• The representation of a given vector in terms of a basis set is unique.
To see this, let {v1, . . . , vk} be a basis for a vector space that includes the
vector x, and let

Now suppose

so that we have

x = c1v1 + ··· ckvk.
x = b1v1 + ··· bkvk,

0 = (c1 − b1)v1 + ··· + (ck − bk)vk.

Since {v1, . . . , vk} are independent, the only way this is possible is if ci = bi
for each i.
A related fact is that if {v1, . . . , vk} is a basis for a vector space of order
n that includes the vector x and x = c1v1 + ··· ckvk, then x = 0n if and only
if ci = 0 for each i.
If B1 is a basis set for V1, B2 is a basis set for V2, and V1 ⊕ V2 = V, then
B1 ∪ B2 is a generating set for V because from the deﬁnition of ⊕ we see that
any vector in V can be represented as a linear combination of vectors in B1
plus a linear combination of vectors in B2.
The number of vectors in a generating set is at least as great as the dimen-
sion of the vector space. Because the vectors in a basis set are independent,

the number of vectors in a basis set is exactly the same as the dimension of
the vector space; that is, if B is a basis set of the vector space V, then

2.1 Operations on Vectors

15

dim(V) = #(B).

(2.8)
A generating set or spanning set of a cone C is a set of vectors S = {vi}
such that for any vector v in C there exists scalars ai ≥ 0 so that v =! aivi,
and if for scalars bi ≥ 0 and! bivi = 0, then bi = 0 for all i. If a generating

set of a cone has a ﬁnite number of elements, the cone is a polyhedron. A
generating set consisting of the minimum number of vectors of any generating
set for that cone is a basis set for the cone.

2.1.4 Inner Products

A useful operation on two vectors x and y of the same order is the dot product,
which we denote by ⟨x, y⟩ and deﬁne as
⟨x, y⟩ ="i

(2.9)

xiyi.

The dot product is also called the inner product or the scalar product. The
dot product is actually a special type of inner product, but it is the most
commonly used inner product, and so we will use the terms synonymously. A
vector space together with an inner product is called an inner product space.
The dot product is also sometimes written as x · y, hence the name. Yet
another notation for the dot product is xTy, and we will see later that this
notation is natural in the context of matrix multiplication. We have the equiv-
alent notations

⟨x, y⟩ ≡ x · y ≡ xTy.

The dot product is a mapping from a vector space V to IR that has the

following properties:
1. Nonnegativity and mapping of the identity:

3. Factoring of scalar multiplication in dot products:

if x ̸= 0, then ⟨x, x⟩ > 0 and ⟨0, x⟩ = ⟨x, 0⟩ = ⟨0, 0⟩ = 0.
2. Commutativity:
⟨x, y⟩ = ⟨y, x⟩.
⟨ax, y⟩ = a⟨x, y⟩ for real a.
⟨x + y, z⟩ = ⟨x, z⟩ + ⟨y, z⟩.

4. Relation of vector addition to addition of dot products:

These properties in fact deﬁne a more general inner product for other kinds of
mathematical objects for which an addition, an additive identity, and a multi-
plication by a scalar are deﬁned. (We should restate here that we assume the
vectors have real elements. The dot product of vectors over the complex ﬁeld
is not an inner product because, if x is complex, we can have xTx = 0 when

16

2 Vectors and Vector Spaces

x ̸= 0. An alternative deﬁnition of a dot product using complex conjugates is
an inner product, however.) Inner products are also deﬁned for matrices, as we
will discuss on page 74. We should note in passing that there are two diﬀerent
kinds of multiplication used in property 3. The ﬁrst multiplication is scalar
multiplication, which we have deﬁned above, and the second multiplication is
ordinary multiplication in IR. There are also two diﬀerent kinds of addition
used in property 4. The ﬁrst addition is vector addition, deﬁned above, and
the second addition is ordinary addition in IR. The dot product can reveal
fundamental relationships between the two vectors, as we will see later.

A useful property of inner products is the Cauchy-Schwarz inequality:

⟨x, y⟩ ≤ ⟨x, x⟩

1

2⟨y, y⟩

1
2 .

(2.10)

This relationship is also sometimes called the Cauchy-Bunyakovskii-Schwarz
inequality. (Augustin-Louis Cauchy gave the inequality for the kind of dis-
crete inner products we are considering here, and Viktor Bunyakovskii and
Hermann Schwarz independently extended it to more general inner products,
deﬁned on functions, for example.) The inequality is easy to see, by ﬁrst ob-
serving that for every real number t,

0 ≤ ⟨(tx + y), (tx + y)⟩2
= ⟨x, x⟩t2 + 2⟨x, y⟩t + ⟨y, y⟩
= at2 + bt + c,

where the constants a, b, and c correspond to the dot products in the preceding
equation. This quadratic in t cannot have two distinct real roots. Hence the
discriminant, b2 − 4ac, must be less than or equal to zero; that is,

2 b$2
#1

≤ ac.

By substituting and taking square roots, we get the Cauchy-Schwarz inequal-
ity. It is also clear from this proof that equality holds only if x = 0 or if y = rx,
for some scalar r.

2.1.5 Norms

We consider a set of objects S that has an addition-type operator, +, a cor-
responding additive identity, 0, and a scalar multiplication; that is, a multi-
plication of the objects by a real (or complex) number. On such a set, a norm
is a function, ∥ · ∥, from S to IR that satisﬁes the following three conditions:
1. Nonnegativity and mapping of the identity:

2. Relation of scalar multiplication to real multiplication:

if x ̸= 0, then ∥x∥ > 0, and ∥0∥ = 0 .
∥ax∥ = |a|∥x∥ for real a.

2.1 Operations on Vectors

17

3. Triangle inequality:

∥x + y∥ ≤ ∥x∥ + ∥y∥.

(If property 1 is relaxed to require only ∥x∥ ≥ 0 for x ̸= 0, the function is
called a seminorm.) Because a norm is a function whose argument is a vector,
we also often use a functional notation such as ρ(x) to represent a norm.

Sets of various types of objects (functions, for example) can have norms,
but our interest in the present context is in norms for vectors and (later)
for matrices. (The three properties above in fact deﬁne a more general norm
for other kinds of mathematical objects for which an addition, an additive
identity, and multiplication by a scalar are deﬁned. Norms are deﬁned for
matrices, as we will discuss later. Note that there are two diﬀerent kinds of
multiplication used in property 2 and two diﬀerent kinds of addition used in
property 3.)

A vector space together with a norm is called a normed space.
For some types of objects, a norm of an object may be called its “length”
or its “size”. (Recall the ambiguity of “length” of a vector that we mentioned
at the beginning of this chapter.)

Lp Norms

There are many norms that could be deﬁned for vectors. One type of norm is
called an Lp norm, often denoted as ∥ · ∥p. For p ≥ 1, it is deﬁned as

∥x∥p =%"i

p

|xi|p& 1

.

(2.11)

This is also sometimes called the Minkowski norm and also the H¨older norm.
It is easy to see that the Lp norm satisﬁes the ﬁrst two conditions above. For
general p ≥ 1 it is somewhat more diﬃcult to prove the triangular inequality
(which for the Lp norms is also called the Minkowski inequality), but for some
special cases it is straightforward, as we will see below.

The most common Lp norms, and in fact the most commonly used vector

norms, are:
•

to sums of distances along coordinate axes, as one would travel along the
rectangular street plan of Manhattan.

i , also called the Euclidean norm, the Euclidean length, or
just the length of the vector. The Lp norm is the square root of the inner

∥x∥1 = !i |xi|, also called the Manhattan norm because it corresponds
∥x∥2 =’!i x2
product of the vector with itself: ∥x∥2 =’⟨x, x⟩.
∥x∥∞ = maxi |xi|, also called the max norm or the Chebyshev norm. The
L∞ norm is deﬁned by taking the limit in an Lp norm, and we see that it
is indeed maxi |xi| by expressing it as

•

•

18

2 Vectors and Vector Spaces

∥x∥∞ = lim

p→∞∥x∥p = lim

p→∞%"i

p

|xi|p& 1

= m lim

p→∞%"i

p

p& 1

(((

xi

m(((

with m = maxi |xi|. Because the quantity of which we are taking the pth
root is bounded above by the number of elements in x and below by 1,
that factor goes to 1 as p goes to ∞.

An Lp norm is also called a p-norm, or 1-norm, 2-norm, or ∞-norm in those
special cases.
It is easy to see that, for any n-vector x, the Lp norms have the relation-

(2.12)
More generally, for given x and for p ≥ 1, we see that ∥x∥p is a nonincreasing
function of p.
We also have bounds that involve the number of elements in the vector:

∥x∥∞ ≤ ∥x∥2 ≤ ∥x∥1.

ships

and

(2.13)

∥x∥∞ ≤ ∥x∥2 ≤ √n∥x∥∞,
∥x∥2 ≤ ∥x∥1 ≤ √n∥x∥2.

L2 norm it can be seen by expanding!(xi + yi)2 and then using the Cauchy-

(2.14)
The triangle inequality obviously holds for the L1 and L∞ norms. For the
Schwarz inequality (2.10) on page 16. Rather than approaching it that way,
however, we will show below that the L2 norm can be deﬁned in terms of an
inner product, and then we will establish the triangle inequality for any norm
deﬁned similarly by an inner product; see inequality (2.19). Showing that the
triangle inequality holds for other Lp norms is more diﬃcult; see Exercise 2.6.
A generalization of the Lp vector norm is the weighted Lp vector norm

deﬁned by

∥x∥wp =%"i

p

wi|xi|p& 1

,

(2.15)

where wi ≥ 0.
Basis Norms
If {v1, . . . , vk} is a basis for a vector space that includes a vector x with
x = c1v1 + ··· + ckvk, then

ρ(x) =%"i

2

i& 1

c2

(2.16)

is a norm. It is straightforward to see that ρ(x) is a norm by checking the
following three conditions:

2.1 Operations on Vectors

19

•

ci = 0 for all i.

• ρ(x) ≥ 0 and ρ(x) = 0 if and only if x = 0 because x = 0 if and only if
i* 1
i* 1
2 = |a|)!i a2c2
• ρ(ax) =)!i a2c2
2 = |a|ρ(x).
If y = b1v1 + ··· + bkvk, then
ρ(x + y) =%"i
i& 1
2%"i
≤%"i
(ci + bi)2& 1
The last inequality is just the triangle inequality for the L2 norm for the
vectors (c1,··· , ck) and (b1,··· , bk).
In Section 2.2.5, we will consider special forms of basis sets in which the
norm in equation (2.16) is identically the L2 norm. (This is called Parseval’s
identity, equation (2.38).)

i& 1

= ρ(x)ρ(y).

c2

b2

2

2

Equivalence of Norms

There is an equivalence among any two norms over a normed linear space in
the sense that if ∥ · ∥a and ∥ · ∥b are norms, then there are positive numbers r
and s such that for any x in the space,

r∥x∥b ≤ ∥x∥a ≤ s∥x∥b.

(2.17)

Expressions (2.13) and (2.14) are examples of this general equivalence for
three Lp norms.

We can prove inequality (2.17) by using the norm deﬁned in equa-
tion (2.16). We need only consider the case x ̸= 0, because the inequality
is obviously true if x = 0. Let ∥ · ∥a be any norm over a given normed linear
space and let {v1, . . . , vk} be a basis for the space. Any x in the space has a
representation in terms of the basis, x = c1v1 + ··· + ckvk. Then

|ci|∥vi∥a.

≤

k"1=i

civi+++++a
∥x∥a =+++++
k"1=i
|ci|∥vi∥a ≤% k"1=i
i& 1
2% k"1=i

c2

k"1=i

Applying the Cauchy-Schwarz inequality to the two vectors (c1,··· , ck) and
(∥v1∥a,··· ,∥vk∥a), we have

2

a& 1
∥vi∥2

.

a) 1

2 , which must be positive, we have

Hence, with ˜s = (!i ∥vi∥2
Now, to establish a lower bound for ∥x∥a, let us deﬁne a subset C of the
linear space consisting of all vectors (u1, . . . , uk) such that!|ui|2 = 1. This

∥x∥a ≤ ˜sρ(x).

20

2 Vectors and Vector Spaces

set is obviously closed. Next, we deﬁne a function f(·) over this closed subset
by

f(u) =+++++
k"1=i

uivi+++++a

.

Because f is continuous, it attains a minimum in this closed subset, say for

the vector u∗; that is, f(u∗) ≤ f(u) for any u such that!|ui|2 = 1. Let
which must be positive, and again consider any x in the normed linear space
and express it in terms of the basis, x = c1v1 + ··· ckvk. If x ̸= 0, we have

˜r = f(u∗),

c2

= ρ(x)f(˜c),

1=i c2

⎛⎜⎝

∥x∥a = ∥

civi∥a

ci

1=i c2

/!k

˜rρ(x) ≤ ∥x∥a ≤ ˜sρ(x).

2+++++++
i& 1
k"1=i

hence, combining this with the inequality above, we have

2⎞⎟⎠ vi+++++++a
i0 1

i )1/2. Because ˜c is in the set C, f(˜c) ≥ r;

k"1=i
=% k"1=i
where ˜c = (c1,··· , ck)/(!k
This expression holds for any norm ∥·∥a and so, after obtaining similar bounds
for any other norm ∥·∥b and then combining the inequalities for ∥·∥a and ∥·∥b,
we have the bounds in the equivalence relation (2.17). (This is an equivalence
relation because it is reﬂexive, symmetric, and transitive. Its transitivity is
seen by the same argument that allowed us to go from the inequalities involv-
ing ρ(·) to ones involving ∥ · ∥b.)
Convergence of Sequences of Vectors
A sequence of real numbers a1, a2, . . . is said to converge to a ﬁnite number a
if for any given ϵ > 0 there is an integer M such that, for k > M, |ak − a| < ϵ,
and we write limk→∞ ak = a, or we write ak → a as k → ∞. If M does not
depend on ϵ, the convergence is said to be uniform.
We deﬁne convergence of a sequence of vectors in terms of the convergence
of a sequence of their norms, which is a sequence of real numbers. We say that
a sequence of vectors x1, x2, . . . (of the same order) converges to the vector x
with respect to the norm ∥ · ∥ if the sequence of real numbers ∥x1 − x∥,∥x2 −
x∥, . . . converges to 0. Because of the bounds (2.17), the choice of the norm is
irrelevant, and so convergence of a sequence of vectors is well-deﬁned without
reference to a speciﬁc norm. (This is the reason equivalence of norms is an
important property.)

Norms Induced by Inner Products

2.1 Operations on Vectors

21

There is a close relationship between a norm and an inner product. For any
inner product space with inner product ⟨·,·⟩, a norm of an element of the
space can be deﬁned in terms of the square root of the inner product of the
element with itself:

(2.18)
Any function ∥ · ∥ deﬁned in this way satisﬁes the properties of a norm. It is
easy to see that ∥x∥ satisﬁes the ﬁrst two properties of a norm, nonnegativity
and scalar equivariance. Now, consider the square of the right-hand side of
the triangle inequality, ∥x∥ + ∥y∥:

∥x∥ =’⟨x, x⟩.

(∥x∥ + ∥y∥)2 = ⟨x, x⟩ + 2’⟨x, x⟩⟨y, y⟩ + ⟨y, y⟩

≥ ⟨x, x⟩ + 2⟨x, y⟩ + ⟨y, y⟩
= ⟨x + y, x + y⟩
= ∥x + y∥2;

(2.19)
hence, the triangle inequality holds. Therefore, given an inner product, ⟨x, y⟩,
then’⟨x, x⟩ is a norm.
Equation (2.18) deﬁnes a norm given any inner product. It is called the
norm induced by the inner product. In the case of vectors and the inner product
we deﬁned for vectors in equation (2.9), the induced norm is the L2 norm, ∥·∥2,
deﬁned above.
In the following, when we use the unqualiﬁed symbol ∥ · ∥ for a vector
norm, we will mean the L2 norm; that is, the Euclidean norm, the induced
norm.

In the sequence of equations above for an induced norm of the sum of two
vectors, one equation (expressed diﬀerently) stands out as particularly useful
in later applications:

∥x + y∥2 = ∥x∥2 + ∥y∥2 + 2⟨x, y⟩.

(2.20)

2.1.6 Normalized Vectors

The Euclidean norm of a vector corresponds to the length of the vector x in a
natural way; that is, it agrees with our intuition regarding “length”. Although,
as we have seen, this is just one of many vector norms, in most applications
it is the most useful one. (I must warn you, however, that occasionally I will
carelessly but naturally use “length” to refer to the order of a vector; that is,
the number of elements. This usage is common in computer software packages
such as R and SAS IML, and software necessarily shapes our vocabulary.)

Dividing a given vector by its length normalizes the vector, and the re-

sulting vector with length 1 is said to be normalized; thus

˜x =

1
∥x∥

x

(2.21)

22

2 Vectors and Vector Spaces

is a normalized vector. Normalized vectors are sometimes referred to as “unit
vectors”, although we will generally reserve this term for a special kind of nor-
malized vector (see page 12). A normalized vector is also sometimes referred
to as a “normal vector”. I use “normalized vector” for a vector such as ˜x in
equation (2.21) and use the latter phrase to denote a vector that is orthogonal
to a subspace.

2.1.7 Metrics and Distances

It is often useful to consider how far apart two vectors are; that is, the “dis-
tance” between them. A reasonable distance measure would have to satisfy
certain requirements, such as being a nonnegative real number. A function ∆
that maps any two objects in a set S to IR is called a metric on S if, for all
x, y, and z in S, it satisﬁes the following three conditions:
1. ∆(x, y) > 0 if x ̸= y and ∆(x, y) = 0 if x = y;
2. ∆(x, y) = ∆(y, x);
3. ∆(x, y) ≤ ∆(x, z) + ∆(z, y).
These conditions correspond in an intuitive manner to the properties we ex-
pect of a distance between objects.

Metrics Induced by Norms

If subtraction and a norm are deﬁned for the elements of S, the most common
way of forming a metric is by using the norm. If ∥ · ∥ is a norm, we can verify
that
(2.22)
is a metric by using the properties of a norm to establish the three properties
of a metric above (Exercise 2.7).

∆(x, y) = ∥x − y∥

The general inner products, norms, and metrics deﬁned above are relevant
in a wide range of applications. The sets on which they are deﬁned can consist
of various types of objects. In the context of real vectors, the most common
inner product is the dot product; the most common norm is the Euclidean
norm that arises from the dot product; and the most common metric is the
one deﬁned by the Euclidean norm, called the Euclidean distance.

2.1.8 Orthogonal Vectors and Orthogonal Vector Spaces

Two vectors v1 and v2 such that

(2.23)
are said to be orthogonal, and this condition is denoted by v1 ⊥ v2. (Some-
times we exclude the zero vector from this deﬁnition, but it is not important

⟨v1, v2⟩ = 0

2.1 Operations on Vectors

23

to do so.) Normalized vectors that are all orthogonal to each other are called
orthonormal vectors. (If the elements of the vectors are from the ﬁeld of com-
plex numbers, orthogonality and normality are deﬁned in terms of the dot
products of a vector with a complex conjugate of a vector.)

A set of nonzero vectors that are mutually orthogonal are necessarily lin-
early independent. To see this, we show it for any two orthogonal vectors
and then indicate the pattern that extends to three or more vectors. Sup-
pose v1 and v2 are nonzero and are orthogonal; that is, ⟨v1, v2⟩ = 0. We see
immediately that if there is a scalar a such that v1 = av2, then a must be
nonzero and we have a contradiction because ⟨v1, v2⟩ = a⟨v1, v1⟩ ̸= 0. For three
mutually orthogonal vectors, v1, v2, and v3, we consider v1 = av2 + bv3 for a
or b nonzero, and arrive at the same contradiction.

Two vector spaces V1 and V2 are said to be orthogonal, written V1 ⊥ V2,
if each vector in one is orthogonal to every vector in the other. If V1 ⊥ V2 and
V1 ⊕ V2 = IRn, then V2 is called the orthogonal complement of V1, and this is
written as V2 = V⊥1 . More generally, if V1 ⊥ V2 and V1 ⊕ V2 = V, then V2 is
called the orthogonal complement of V1 with respect to V. This is obviously
a symmetric relationship; if V2 is the orthogonal complement of V1, then V1
is the orthogonal complement of V2.
If B1 is a basis set for V1, B2 is a basis set for V2, and V2 is the orthogonal
complement of V1 with respect to V, then B1 ∪ B2 is a basis set for V. It is
a basis set because since V1 and V2 are orthogonal, it must be the case that
B1 ∩ B2 = ∅.

If V1 ⊂ V, V2 ⊂ V, V1 ⊥ V2, and dim(V1) + dim(V2) = dim(V), then

V1 ⊕ V2 = V;

(2.24)
that is, V2 is the orthogonal complement of V1. We see this by ﬁrst letting B1
and B2 be bases for V1 and V2. Now V1 ⊥ V2 implies that B1 ∩ B2 = ∅ and
dim(V1) + dim(V2) = dim(V) implies #(B1) + #(B2) = #(B), for any basis
set B for V; hence, B1 ∪ B2 is a basis set for V.
The intersection of two orthogonal vector spaces consists only of the zero
vector (Exercise 2.9).

A set of linearly independent vectors can be mapped to a set of mutu-
ally orthogonal (and orthonormal) vectors by means of the Gram-Schmidt
transformations (see equation (2.34) below).

2.1.9 The “One Vector”

Another often useful vector is the vector with all elements equal to 1. We call
this the “one vector” and denote it by 1 or by 1n. The one vector can be used
in the representation of the sum of the elements in a vector:

The one vector is also called the “summing vector”.

1Tx =" xi.

(2.25)

24

2 Vectors and Vector Spaces

The Mean and the Mean Vector

Because the elements of x are real, they can be summed; however, in applica-
tions it may or may not make sense to add the elements in a vector, depending
on what is represented by those elements. If the elements have some kind of
essential commonality, it may make sense to compute their sum as well as
their arithmetic mean, which for the n-vector x is denoted by ¯x and deﬁned
by

(2.26)
We also refer to the arithmetic mean as just the “mean” because it is the most
commonly used mean.

¯x = 1T

n x/n.

It is often useful to think of the mean as an n-vector all of whose elements

are ¯x. The symbol ¯x is also used to denote this vector; hence, we have

¯x = ¯x1n,

(2.27)

in which ¯x on the left-hand side is a vector and ¯x on the right-hand side is a
scalar. We also have, for the two diﬀerent objects,

∥¯x∥2 = n¯x2.

(2.28)

The meaning, whether a scalar or a vector, is usually clear from the con-
text. In any event, an expression such as x − ¯x is unambiguous; the addition
(subtraction) has the same meaning whether ¯x is interpreted as a vector or a
scalar. (In some mathematical treatments of vectors, addition of a scalar to
a vector is not deﬁned, but here we are following the conventions of modern
computer languages.)

2.2 Cartesian Coordinates and Geometrical
Properties of Vectors

Points in a Cartesian geometry can be identiﬁed with vectors. Several deﬁ-
nitions and properties of vectors can be motivated by this geometric inter-
pretation. In this interpretation, vectors are directed line segments with a
common origin. The geometrical properties can be seen most easily in terms
of a Cartesian coordinate system, but the properties of vectors deﬁned in
terms of a Cartesian geometry have analogues in Euclidean geometry without
a coordinate system. In such a system, only length and direction are deﬁned,
and two vectors are considered to be the same vector if they have the same
length and direction. Generally, we will not assume that there is a “position”
associated with a vector.

2.2 Cartesian Geometry

25

2.2.1 Cartesian Geometry

A Cartesian coordinate system in d dimensions is deﬁned by d unit vectors,
ei in equation (2.3), each with d elements. A unit vector is also called a
principal axis of the coordinate system. The set of unit vectors is orthonormal.
(There is an implied number of elements of a unit vector that is inferred from
the context. Also parenthetically, we remark that the phrase “unit vector” is
sometimes used to refer to a vector the sum of whose squared elements is 1,
that is, whose length, in the Euclidean distance sense, is 1. As we mentioned
above, we refer to this latter type of vector as a “normalized vector”.)

The sum of all of the unit vectors is the one vector:

ei = 1d.

d"1=1

A point x with Cartesian coordinates (x1, . . . , xd) is associated with a
vector from the origin to the point, that is, the vector (x1, . . . , xd). The vector
can be written as the linear combination

x = x1e1 + . . . + xded

or, equivalently, as

x = ⟨x, e1⟩e1 + . . . + ⟨x, ed⟩en.
(This is a Fourier expansion, equation (2.36) below.)

2.2.2 Projections

The projection of the vector y onto the vector x is the vector

ˆy = ⟨x, y⟩
∥x∥2 x.

(2.29)

This deﬁnition is consistent with a geometrical interpretation of vectors as
directed line segments with a common origin. The projection of y onto x is
the inner product of the normalized x and y times the normalized x; that is,
⟨˜x, y⟩˜x, where ˜x = x/∥x∥. Notice that the order of y and x is the same.
An important property of a projection is that when it is subtracted from
the vector that was projected, the resulting vector, called the “residual”, is
orthogonal to the projection; that is, if

r = y − ⟨x, y⟩
∥x∥2 x
= y − ˆy

(2.30)

then r and ˆy are orthogonal, as we can easily see by taking their inner product
(see Figure 2.1). Notice also that the Pythagorean relationship holds:

26

2 Vectors and Vector Spaces

y

r

y^

x

θ

x

θ

y

r

y^

Fig. 2.1. Projections and Angles

∥y∥2 = ∥ˆy∥2 + ∥r∥2.

(2.31)

As we mentioned on page 24, the mean ¯y can be interpreted either as a
scalar or as a vector all of whose elements are ¯y. As a vector, it is the projection
of y onto the one vector 1n,

⟨1n, y⟩
∥1n∥2 1n =

1T
n y
n

1n

= ¯y 1n,

from equations (2.26) and (2.29).

We will consider more general projections (that is, projections onto planes
or other subspaces) on page 280, and on page 331 we will view linear regression
ﬁtting as a projection onto the space spanned by the independent variables.

2.2.3 Angles between Vectors

The angle between the vectors x and y is determined by its cosine, which we
can compute from the length of the projection of one vector onto the other.
Hence, denoting the angle between x and y as angle(x, y), we deﬁne

angle(x, y) = cos−1# ⟨x, y⟩
∥x∥∥y∥$ ,

(2.32)

with cos−1(·) being taken in the interval [0, π]. The cosine is ±∥ˆy∥/∥y∥, with
the sign chosen appropriately; see Figure 2.1. Because of this choice of cos−1(·),
we have that angle(y, x) = angle(x, y) — but see Exercise 2.13e on page 39.
The word “orthogonal” is appropriately deﬁned by equation (2.23) on
page 22 because orthogonality in that sense is equivalent to the corresponding
geometric property. (The cosine is 0.)

2.2 Cartesian Geometry

27

Notice that the angle between two vectors is invariant to scaling of the

vectors; that is, for any nonzero scalar a, angle(ax, y) = angle(x, y).

A given vector can be deﬁned in terms of its length and the angles θi that
it makes with the unit vectors. The cosines of these angles are just the scaled
coordinates of the vector:

cos(θi) = ⟨x, ei⟩
∥x∥∥ei∥
1
xi.
∥x∥

=

(2.33)

These quantities are called the direction cosines of the vector.

Although geometrical intuition often helps us in understanding properties
of vectors, sometimes it may lead us astray in high dimensions. Consider the
direction cosines of an arbitrary vector in a vector space with large dimensions.
If the elements of the arbitrary vector are nearly equal (that is, if the vector is
a diagonal through an orthant of the coordinate system), the direction cosine
goes to 0 as the dimension increases. In high dimensions, any two vectors are
“almost orthogonal” to each other; see Exercise 2.11.

The geometric property of the angle between vectors has important im-
plications for certain operations both because it may indicate that rounding
in computations will have deleterious eﬀects and because it may indicate a
deﬁciency in the understanding of the application.

We will consider more general projections and angles between vectors and
other subspaces on page 287. In Section 5.2.1, we will consider rotations of
vectors onto other vectors or subspaces. Rotations are similar to projections,
except that the length of the vector being rotated is preserved.

2.2.4 Orthogonalization Transformations

Given m nonnull, linearly independent vectors, x1, . . . , xm, it is easy to form
m orthonormal vectors, ˜x1, . . . , ˜xm, that span the same space. A simple way
to do this is sequentially. First normalize x1 and call this ˜x1. Next, project x2
onto ˜x1 and subtract this projection from x2. The result is orthogonal to ˜x1;
hence, normalize this and call it ˜x2. These ﬁrst two steps, which are illustrated
in Figure 2.2, are

1
∥x1∥

x1,

1

˜x1 =

˜x2 =

∥x2 − ⟨˜x1, x2⟩˜x1∥

(x2 − ⟨˜x1, x2⟩˜x1).

(2.34)

These are called Gram-Schmidt transformations.

The Gram-Schmidt transformations can be continued with all of the vec-
tors in the linearly independent set. There are two straightforward ways equa-
tions (2.34) can be extended. One method generalizes the second equation in

28

2 Vectors and Vector Spaces

x~
2

x2

x2 − p

x~
1

x1

p

projection onto

x~

1

Fig. 2.2. Orthogonalization of x1 and x2

an obvious way:

for k = 2, 3 . . . ,

˜xk =%xk −

⟨˜xi, xk⟩˜xi& 4+++++
k−1"i=1

xk −

k−1"i=1

⟨˜xi, xk⟩˜xi+++++

.

(2.35)

In this method, at the kth step, we orthogonalize the kth vector by comput-
ing its residual with respect to the plane formed by all the previous k − 1
orthonormal vectors.
Another way of extending the transformation of equations (2.34) is, at
the kth step, to compute the residuals of all remaining vectors with respect
just to the kth normalized vector. We describe this method explicitly in
Algorithm 2.1.
Algorithm 2.1 Gram-Schmidt Orthonormalization of a Set of
Linearly Independent Vectors, x1, . . . , xm
0. For k = 1, . . . , m,

2. If m > 1, for k = 2, . . . , m,

{set ˜xi = xi.
}1. Ensure that ˜x1 ̸= 0;
set ˜x1 = ˜x1/∥˜x1∥.
{ for j = k, . . . , m,
{ set ˜xj = ˜xj − ⟨˜xk−1, ˜xj⟩˜xk−1.
}

2.2 Cartesian Geometry

29

ensure that ˜xk ̸= 0;
set ˜xk = ˜xk/∥˜xk∥.

}
Although the method indicated in equation (2.35) is mathematically equiv-
alent to this method, the use of Algorithm 2.1 is to be preferred for com-
putations because it is less subject to rounding errors. (This may not be
immediately obvious, although a simple numerical example can illustrate the
fact — see Exercise 11.1c on page 441. We will not digress here to consider this
further, but the diﬀerence in the two methods has to do with the relative mag-
nitudes of the quantities in the subtraction. The method of Algorithm 2.1 is
sometimes called the “modiﬁed Gram-Schmidt method”. We will discuss this
method again in Section 11.2.1.) This is an instance of a principle that we will
encounter repeatedly: the form of a mathematical expression and the way the
expression should be evaluated in actual practice may be quite diﬀerent.

These orthogonalizing transformations result in a set of orthogonal vectors
that span the same space as the original set. They are not unique; if the order
in which the vectors are processed is changed, a diﬀerent set of orthogonal
vectors will result.

Orthogonal vectors are useful for many reasons: perhaps to improve the
stability of computations; or in data analysis to capture the variability most
eﬃciently; or for dimension reduction as in principal components analysis; or
in order to form more meaningful quantities as in a vegetative index in remote
sensing. We will discuss various speciﬁc orthogonalizing transformations later.

2.2.5 Orthonormal Basis Sets

A basis for a vector space is often chosen to be an orthonormal set because it
is easy to work with the vectors in such a set.

If u1, . . . , un is an orthonormal basis set for a space, then a vector x in

that space can be expressed as

and because of orthonormality, we have

x = c1u1 + ··· + cnun,

ci = ⟨x, ui⟩.

(2.36)

(2.37)

(We see this by taking the inner product of both sides with ui.) A represen-
tation of a vector as a linear combination of orthonormal basis vectors, as in
equation (2.36), is called a Fourier expansion, and the ci are called Fourier
coeﬃcients.

By taking the inner product of each side of equation (2.36) with itself, we

have Parseval’s identity:

∥x∥2 =" c2

i .

(2.38)

30

2 Vectors and Vector Spaces

This shows that the L2 norm is the same as the norm in equation (2.16) (on
page 18) for the case of an orthogonal basis.

Although the Fourier expansion is not unique because a diﬀerent orthog-
onal basis set could be chosen, Parseval’s identity removes some of the arbi-
trariness in the choice; no matter what basis is used, the sum of the squares of
the Fourier coeﬃcients is equal to the square of the norm that arises from the
inner product. (“The” inner product means the inner product used in deﬁning
the orthogonality.)

Another useful expression of Parseval’s identity in the Fourier expansion is

x −

k"i=1

+++++

2

ciui+++++

= ⟨x, x⟩ −

c2
i

k"i=1

(2.39)

(because the term on the left-hand side is 0).

The expansion (2.36) is a special case of a very useful expansion in an
orthogonal basis set. In the ﬁnite-dimensional vector spaces we consider here,
the series is ﬁnite. In function spaces, the series is generally inﬁnite, and so
issues of convergence are important. For diﬀerent types of functions, diﬀerent
orthogonal basis sets may be appropriate. Polynomials are often used, and
there are some standard sets of orthogonal polynomials, such as Jacobi, Her-
mite, and so on. For periodic functions especially, orthogonal trigonometric
functions are useful.

2.2.6 Approximation of Vectors

In high-dimensional vector spaces, it is often useful to approximate a given
vector in terms of vectors from a lower dimensional space. Suppose, for exam-
ple, that V ⊂ IRn is a vector space of dimension k (necessarily, k ≤ n) and x is
a given n-vector. We wish to determine a vector ˜x in V that approximates x.
Optimality of the Fourier Coeﬃcients

The ﬁrst question, of course, is what constitutes a “good” approximation. One
obvious criterion would be based on a norm of the diﬀerence of the given vector
and the approximating vector. So now, choosing the norm as the Euclidean
norm, we may pose the problem as one of ﬁnding ˜x ∈ V such that

∥x − ˜x∥ ≤ ∥x − v∥ ∀ v ∈ V.

(2.40)

This diﬀerence is a truncation error. Let u1, . . . , uk be an orthonormal basis
set for V, and let

(2.41)
where the ci are the Fourier coeﬃcients of x, ⟨x, ui⟩. Now let v = a1u1 +··· +
akuk be any other vector in V, and consider

˜x = c1u1 + ··· + ckuk,

2.2 Cartesian Geometry

31

c2
i −

c2
i

k"i=1

(2.42)

2

x −

aiui, x −

aiui+++++
∥x − v∥2 =+++++
k"i=1
=5x −
k"i=1
k"i=1
k"i=1
= ⟨x, x⟩ − 2
ai⟨x, ui⟩ +
k"i=1
k"i=1
= ⟨x, x⟩ − 2
k"i=1
(ai − ci)2 −
= ⟨x, x⟩ +
ciui+++++
k"i=1
k"i=1
ciui+++++
k"i=1

=+++++
≥+++++

aici +

x −

x −

2

2

+

.

aiui6
k"i=1

i +
a2

a2
i

k"i=1

c2
i

k"i=1
(ai − ci)2

Therefore we have ∥x − ˜x∥ ≤ ∥x − v∥, and so ˜x is the best approximation of
x with respect to the Euclidean norm in the k-dimensional vector space V.
Choice of the Best Basis Subset

Now, posing the problem another way, we may seek the best k-dimensional
subspace of IRn from which to choose an approximating vector. This question
is not well-posed (because the one-dimensional vector space determined by x
is the solution), but we can pose a related interesting question: suppose we
have a Fourier expansion of x in terms of a set of n orthogonal basis vectors,
u1, . . . , un, and we want to choose the “best” k basis vectors from this set and
use them to form an approximation of x. (This restriction of the problem is
equivalent to choosing a coordinate system.) We see the solution immediately
from inequality (2.42): we choose the k uis corresponding to the k largest cis
in absolute value, and we take

˜x = ci1ui1 + ··· + cik uik ,

(2.43)

where min({|cij| : j = 1, . . . , k}) ≥ max({|cij| : j = k + 1, . . . , n}).
2.2.7 Flats, Aﬃne Spaces, and Hyperplanes

Given an n-dimensional vector space of order n, IRn for example, consider a
system of m linear equations in the n-vector variable x,

32

2 Vectors and Vector Spaces

1 x = b1
cT
...
...

mx = bm,
cT

where c1, . . . , cm are linearly independent n-vectors (and hence m ≤ n). The
set of points deﬁned by these linear equations is called a ﬂat. Although it is not
necessarily a vector space, a ﬂat is also called an aﬃne space. An intersection
of two ﬂats is a ﬂat.

If the equations are homogeneous (that is, if b1 = ··· = bm = 0), then the
point (0, . . . , 0) is included, and the ﬂat is an (n − m)-dimensional subspace
(also a vector space, of course). Stating this another way, a ﬂat through the
origin is a vector space, but other ﬂats are not vector spaces.

If m = 1, the ﬂat is called a hyperplane. A hyperplane through the origin

is an (n − 1)-dimensional vector space.
vector space.

If m = n−1, the ﬂat is a line. A line through the origin is a one-dimensional

2.2.8 Cones

A cone is an important type of vector set (see page 14 for deﬁnitions). The
most important type of cone is a convex cone, which corresponds to a solid
geometric object with a single ﬁnite vertex.

Given a set of vectors V (usually but not necessarily a cone), the dual cone

of V , denoted V ∗, is deﬁned as

V ∗ = {y∗ s.t. y∗Ty ≥ 0 for all y ∈ V },

and the polar cone of V , denoted V 0, is deﬁned as

V 0 = {y0 s.t. y0Ty ≤ 0 for all y ∈ V }.

Obviously, V 0 can be formed by multiplying all of the vectors in V ∗ by −1,
and so we write V 0 = −V ∗, and we also have (−V )∗ = −V ∗.
Although the deﬁnitions can apply to any set of vectors, dual cones and
polar cones are of the most interest in the case in which the underlying set
of vectors is a cone in the nonnegative orthant (the set of all vectors all of
whose elements are nonnegative). In that case, the dual cone is just the full
nonnegative orthant, and the polar cone is just the nonpositive orthant (the
set of all vectors all of whose elements are nonpositive).

Although a convex cone is not necessarily a vector space, the union of the
dual cone and the polar cone of a convex cone is a vector space. (You are
asked to prove this in Exercise 2.12.) The nonnegative orthant, which is an
important convex cone, is its own dual.

Geometrically, the dual cone V ∗ of V consists of all vectors that form
nonobtuse angles with the vectors in V . Convex cones, dual cones, and polar
cones play important roles in optimization.

2.3 Variances and Covariances

33

2.2.9 Cross Products in IR3

For the special case of the vector space IR3, another useful vector product is
the cross product, which is a mapping from IR3×IR3 to IR3. Before proceeding,
we note an overloading of the term “cross product” and of the symbol “×”
used to denote it. If A and B are sets, the set cross product or the set Cartesian
product of A and B is the set consisting of all doubletons (a, b) where a ranges
over all elements of A, and b ranges independently over all elements of B.
Thus, IR3 × IR3 is the set of all pairs of all real 3-vectors.

The vector cross product of the vectors

x = (x1, x2, x3),
y = (y1, y2, y3),

written x × y, is deﬁned as

x × y = (x2y3 − x3y2, x3y1 − x1y3, x1y2 − x2y1).

(2.44)

(We also use the term “cross products” in a diﬀerent way to refer to another
type of product formed by several inner products; see page 287.) The cross
product has the following properties, which are immediately obvious from the
deﬁnition:
1. Self-nilpotency:

3. Factoring of scalar multiplication;

x × x = 0, for all x.
2. Anti-commutativity:
x × y = −y × x.
ax × y = a(x × y) for real a.
(x + y) × z = (x × z) + (y × z).

4. Relation of vector addition to addition of cross products:

The cross product is useful in modeling phenomena in nature, which are of-
ten represented as vectors in IR3. The cross product is also useful in “three-
dimensional” computer graphics for determining whether a given surface is
visible from a given perspective and for simulating the eﬀect of lighting on a
surface.

2.3 Centered Vectors and Variances
and Covariances of Vectors

In this section, we deﬁne some scalar-valued functions of vectors that are
analogous to functions of random variables averaged over their probabilities or
probability density. The functions of vectors discussed here are the same as the
ones that deﬁne sample statistics. This short section illustrates the properties

34

2 Vectors and Vector Spaces

of norms, inner products, and angles in terms that should be familiar to the
reader.

These functions, and transformations using them, are useful for appli-
cations in the data sciences. It is important to know the eﬀects of various
transformations of data on data analysis.

2.3.1 The Mean and Centered Vectors

When the elements of a vector have some kind of common interpretation, the
sum of the elements or the mean (equation (2.26)) of the vector may have
meaning. In this case, it may make sense to center the vector; that is, to
subtract the mean from each element. For a given vector x, we denote its
centered counterpart as xc:

We refer to any vector whose sum of elements is 0 as a centered vector.

From the deﬁnitions, it is easy to see that

xc = x − ¯x.

(2.45)

(x + y)c = xc + yc

(2.46)

(see Exercise 2.14). Interpreting ¯x as a vector, and recalling that it is the
projection of x onto the one vector, we see that xc is the residual in the
sense of equation (2.30). Hence, we see that xc and x are orthogonal, and the
Pythagorean relationship holds:

∥x∥2 = ∥¯x∥2 + ∥xc∥2.

(2.47)

From this we see that the length of a centered vector is less than or equal to the
length of the original vector. (Notice that equation (2.47) is just the formula

familiar to data analysts, which with some rearrangement is !(xi − ¯x)2 =
! x2
i − n¯x2.)
For any scalar a and n-vector x, expanding the terms, we see that

∥x − a∥2 = ∥xc∥2 + n(a − ¯x)2,

(2.48)

where we interpret ¯x as a scalar here.

Notice that a nonzero vector when centered may be the zero vector. This
leads us to suspect that some properties that depend on a dot product are
not invariant to centering. This is indeed the case. The angle between two
vectors, for example, is not invariant to centering; that is, in general,

angle(xc, yc) ̸= angle(x, y)

(2.49)

(see Exercise 2.15).

2.3 Variances and Covariances

35

2.3.2 The Standard Deviation, the Variance, and Scaled Vectors

We also sometimes ﬁnd it useful to scale a vector by both its length (normalize
the vector) and by a function of its number of elements. We denote this scaled
vector as xs and deﬁne it as

xs = √n − 1 x
∥xc∥

.

(2.50)

For comparing vectors, it is usually better to center the vectors prior to any
scaling. We denote this centered and scaled vector as xcs and deﬁne it as

xcs = √n − 1 xc
∥xc∥

.

(2.51)

Centering and scaling is also called standardizing. Note that the vector is
centered before being scaled. The angle between two vectors is not changed
by scaling (but, of course, it may be changed by centering).

The multiplicative inverse of the scaling factor,

sx = ∥xc∥/√n − 1,

(2.52)

is called the standard deviation of the vector x. The standard deviation of xc
is the same as that of x; in fact, the standard deviation is invariant to the
addition of any constant. The standard deviation is a measure of how much
the elements of the vector vary. If all of the elements of the vector are the
same, the standard deviation is 0 because in that case xc = 0.

The square of the standard deviation is called the variance, denoted by V:

V(x) = s2
x

= ∥xc∥2
n − 1 .
(In perhaps more familiar notation, equation (2.53) is just V(x) = !(xi −
¯x)2/(n − 1).) From equation (2.45), we see that

(2.53)

(The terms “mean”, “standard deviation”, “variance”, and other terms we will
mention below are also used in an analogous, but slightly diﬀerent, manner to
refer to properties of random variables. In that context, the terms to refer to
the quantities we are discussing here would be preceded by the word “sample”,
and often for clarity I will use the phrases “sample standard deviation” and
“sample variance” to refer to what is deﬁned above, especially if the elements
of x are interpreted as independent realizations of a random variable. Also,
recall the two possible meanings of “mean”, or ¯x; one is a vector, and one is
a scalar, as in equation (2.27).)

V(x) =

1

n − 1)∥x∥2 − ∥¯x∥2* .

36

2 Vectors and Vector Spaces

If a and b are scalars (or b is a vector with all elements the same), the

deﬁnition, together with equation (2.48), immediately gives

V(ax + b) = a2V(x).

This implies that for the scaled vector xs,

V(xs) = 1.

If a is a scalar and x and y are vectors with the same number of elements,
from the equation above, and using equation (2.20) on page 21, we see that
the variance following an axpy operation is given by

V(ax + y) = a2V(x) + V(y) + 2a⟨xc, yc⟩
n − 1 .

(2.54)

While equation (2.53) appears to be relatively simple, evaluating the ex-
pression for a given x may not be straightforward. We discuss computational
issues for this expression on page 410. This is an instance of a principle that we
will encounter repeatedly: the form of a mathematical expression and the way
the expression should be evaluated in actual practice may be quite diﬀerent.

2.3.3 Covariances and Correlations between Vectors

If x and y are n-vectors, the covariance between x and y is

Cov(x, y) = ⟨x − ¯x, y − ¯y⟩

.

(2.55)

n − 1

By representing x − ¯x as x − ¯x1 and y − ¯y similarly, and expanding, we see
that Cov(x, y) = (⟨x, y⟩ − n¯x¯y)/(n − 1). Also, we see from the deﬁnition of
covariance that Cov(x, x) is the variance of the vector x, as deﬁned above.
From the deﬁnition and the properties of an inner product given on

for any scalar a (where 1 is the one vector);

page 15, if x, y, and z are conformable vectors, we see immediately that
• Cov(a1, y) = 0
• Cov(ax, y) = aCov(x, y)
• Cov(y, x) = Cov(x, y);
• Cov(y, y) = V(y); and
• Cov(x + z, y) = Cov(x, y) + Cov(z, y),

for any scalar a;

in particular,
– Cov(x + y, y) = Cov(x, y) + V(y), and
– Cov(x + a, y) = Cov(x, y)

for any scalar a.

Exercises

37

Using the deﬁnition of the covariance, we can rewrite equation (2.54) as

V(ax + y) = a2V(x) + V(y) + 2aCov(x, y).

(2.56)

The covariance is a measure of the extent to which the vectors point in
the same direction. A more meaningful measure of this is obtained by the
covariance of the centered and scaled vectors. This is the correlation between
the vectors,

Corr(x, y) = Cov(xcs, ycs)
yc

∥yc∥8

,

=7 xc

∥xc∥
= ⟨xc, yc⟩
∥xc∥∥yc∥

,

(2.57)

which we see immediately from equation (2.32) is the cosine of the angle
between xc and yc:

Corr(x, y) = cos(angle(xc, yc)).

(2.58)

(Recall that this is not the same as the angle between x and y.)

An equivalent expression for the correlation is

Corr(x, y) =

.

(2.59)

Cov(x, y)

’V(x)V(y)

It is clear that the correlation is in the interval [−1, 1] (from the Cauchy-
Schwarz inequality). A correlation of −1 indicates that the vectors point in
opposite directions, a correlation of 1 indicates that the vectors point in the
same direction, and a correlation of 0 indicates that the vectors are orthogonal.
While the covariance is equivariant to scalar multiplication, the absolute
value of the correlation is invariant to it; that is, the correlation changes only
as the sign of the scalar multiplier,

Corr(ax, y) = sign(a)Corr(x, y),

(2.60)

for any scalar a.

Exercises

2.1. Write out the step-by-step proof that the maximum number of n-vectors
that can form a set that is linearly independent is n, as stated on page 11.
2.2. Give an example of two vector spaces whose union is not a vector space.

38

2 Vectors and Vector Spaces

2.3. Let {vi}n

i=1 be an orthonormal basis for the n-dimensional vector space

V. Let x ∈ V have the representation

Show that the Fourier coeﬃcients bi can be computed as

x =" bivi.
bi = ⟨x, vi⟩.

ρ(x) =% n"i=1

|xi|1/2&2

.

2.4. Let p = 1

2 in equation (2.11); that is, let ρ(x) be deﬁned for the n-vector

x as

2.5. Prove equation (2.12) and show that the bounds are sharp by exhibiting

Show that ρ(·) is not a norm.
instances of equality. (Use the fact that ∥x∥∞ = maxi |xi|.)
a) Prove H¨older’s inequality: for any p and q such that p ≥ 1 and

2.6. Prove the following inequalities.

p + q = pq, and for vectors x and y of the same order,

⟨x, y⟩ ≤ ∥x∥p∥y∥q.

b) Prove the triangle inequality for any Lp norm. (This is sometimes

called Minkowski’s inequality.)

Hint: Use H¨older’s inequality.

2.7. Show that the expression deﬁned in equation (2.22) on page 22 is a

metric.

of the zero vector.

2.8. Show that equation (2.31) on page 26 is correct.
2.9. Show that the intersection of two orthogonal vector spaces consists only

2.10. From the deﬁnition of direction cosines in equation (2.33), it is easy to
see that the sum of the squares of the direction cosines is 1. For the
special case of IR3, draw a sketch and use properties of right triangles
to show this geometrically.

2.11. In IR2 with a Cartesian coordinate system, the diagonal directed line
segment through the positive quadrant (orthant) makes a 45◦ angle
with each of the positive axes. In 3 dimensions, what is the angle be-
tween the diagonal and each of the positive axes? In 10 dimensions? In
100 dimensions? In 1000 dimensions? We see that in higher dimensions
any two lines are almost orthogonal. (That is, the angle between them
approaches 90◦.) What are some of the implications of this for data
analysis?

2.12. Show that if C is a convex cone, then C∗ ∪ C0 together with the usual
operations is a vector space, where C∗ is the dual of C and C0 is the

polar cone of C.

Exercises

39

Hint: Just apply the deﬁnitions of the individual terms.

2.13. IR3 and the cross product.

a) Is the cross product associative? Prove or disprove.
b) For x, y ∈ IR3, show that the area of the triangle with vertices
c) For x, y, z ∈ IR3, show that

(0, 0, 0), x, and y is ∥x × y∥/2.

⟨x, y × z⟩ = ⟨x × y, z⟩.

This is called the “triple scalar product”.

d) For x, y, z ∈ IR3, show that

x × (y × z) = ⟨x, z⟩y − ⟨x, y⟩z.

This is called the “triple vector product”. It is in the plane deter-
mined by y and z.

e) The magnitude of the angle between two vectors is determined by
the cosine, formed from the inner product. Show that in the special
case of IR3, the angle is also determined by the sine and the cross
product, and show that this method can determine both the mag-
nitude and the direction of the angle; that is, the way a particular
vector is rotated into the other.

2.14. Using equations (2.26) and (2.45), establish equation (2.46).
2.15. Show that the angle between the centered vectors xc and yc is not the
same in general as the angle between the uncentered vectors x and y of
the same order.

2.16. Formally prove equation (2.54) (and hence equation (2.56)).
2.17. Prove that for any vectors x and y of the same order,

(Cov(x, y))2 ≤ V(x)V(y).

3

Basic Properties of Matrices

In this chapter, we build on the notation introduced on page 5, and discuss
a wide range of basic topics related to matrices with real elements. Some of
the properties carry over to matrices with complex elements, but the reader
should not assume this. Occasionally, for emphasis, we will refer to “real”
matrices, but unless it is stated otherwise, we are assuming the matrices are
real.

The topics and the properties of matrices that we choose to discuss are
motivated by applications in the data sciences. In Chapter 8, we will consider
in more detail some special types of matrices that arise in regression analysis
and multivariate data analysis, and then in Chapter 9 we will discuss some
speciﬁc applications in statistics.

3.1 Basic Deﬁnitions and Notation

It is often useful to treat the rows or columns of a matrix as vectors. Terms
such as linear independence that we have deﬁned for vectors also apply to
rows and/or columns of a matrix. The vector space generated by the columns
of the n × m matrix A is of order n and of dimension m or less, and is called
the column space of A, the range of A, or the manifold of A. This vector space
is denoted by

or

V(A)
span(A).

(The argument of V(·) or span(·) can be either a matrix or a set of vectors.
Recall from Section 2.1.3 that if G is a set of vectors, the symbol span(G)
denotes the vector space generated by the vectors in G.) We also deﬁne the
row space of A to be the vector space of order m (and of dimension n or
less) generated by the rows of A; notice, however, the preference given to the
column space.

42

3 Basic Properties of Matrices

Many of the properties of matrices that we discuss hold for matrices with
an inﬁnite number of elements, but throughout this book we will assume that
the matrices have a ﬁnite number of elements, and hence the vector spaces
are of ﬁnite order and have a ﬁnite number of dimensions.

Similar to our deﬁnition of multiplication of a vector by a scalar, we deﬁne

the multiplication of a matrix A by a scalar c as

cA = (caij).

The aii elements of a matrix are called diagonal elements; an element
aij with i < j is said to be “above the diagonal”, and one with i > j is
said to be “below the diagonal”. The vector consisting of all of the aii’s is
called the principal diagonal or just the diagonal. The elements ai,i+ck are
called “codiagonals” or “minor diagonals”. If the matrix has m columns, the
ai,m+1−i elements of the matrix are called skew diagonal elements. We use
terms similar to those for diagonal elements for elements above and below
the skew diagonal elements. These phrases are used with both square and
nonsquare matrices.

If, in the matrix A with elements aij for all i and j, aij = aji, A is said
to be symmetric. A symmetric matrix is necessarily square. A matrix A such
that aij = −aji is said to be skew symmetric. The diagonal entries of a skew
symmetric matrix must be 0. If aij = ¯aji (where ¯a represents the conjugate
of the complex number a), A is said to be Hermitian. A Hermitian matrix is
also necessarily square, and, of course, a real symmetric matrix is Hermitian.
A Hermitian matrix is also called a self-adjoint matrix.

Many matrices of interest are sparse; that is, they have a large proportion
of elements that are 0. (“A large proportion” is subjective, but generally means
more than 75%, and in many interesting cases is well over 95%.) Eﬃcient
and accurate computations often require that the sparsity of a matrix be
accommodated explicitly.

If all except the principal diagonal elements of a matrix are 0, the matrix
is called a diagonal matrix. A diagonal matrix is the most common and most
important type of sparse matrix. If all of the principal diagonal elements of a
matrix are 0, the matrix is called a hollow matrix. A skew symmetric matrix
is hollow, for example. If all except the principal skew diagonal elements of a
matrix are 0, the matrix is called a skew diagonal matrix.

An n × m matrix A for which
m!j̸=i

|aii| >

|aij|

for each i = 1, . . . , n

(3.1)

is said to be row diagonally dominant; one for which |ajj| >"n

i̸=j |aij| for each
j = 1, . . . , m is said to be column diagonally dominant. (Some authors refer
to this as strict diagonal dominance and use “diagonal dominance” without
qualiﬁcation to allow the possibility that the inequalities in the deﬁnitions

3.1 Basic Deﬁnitions and Notation

43

are not strict.) Most interesting properties of such matrices hold whether the
dominance is by row or by column. If A is symmetric, row and column di-
agonal dominances are equivalent, so we refer to row or column diagonally
dominant symmetric matrices without the qualiﬁcation; that is, as just diag-
onally dominant.

If all elements below the diagonal are 0, the matrix is called an upper
triangular matrix; and a lower triangular matrix is deﬁned similarly.
If all
elements of a column or row of a triangular matrix are zero, we still refer to the
matrix as triangular, although sometimes we speak of its form as trapezoidal.
Another form called trapezoidal is one in which there are more columns than
rows, and the additional columns are possibly nonzero. The four general forms
of triangular or trapezoidal matrices are shown below.

⎡⎣

X X X
0 X X

0 0 X⎤⎦ ⎡⎣

X X X
0 X X

0 0 0⎤⎦

⎡⎣

X X X X
0 X X X

0 0 X X⎤⎦

X X X
0 X X
0 0 X
0 0 0

⎡⎢⎢⎣

⎤⎥⎥⎦

In this notation, X indicates that the element is possibly not zero. It does
not mean each element is the same. In other cases, X and 0 may indicate
“submatrices”, which we discuss in the section on partitioned matrices.

If all elements are 0 except ai,i+ck for some small number of integers ck,
the matrix is called a band matrix (or banded matrix). In many applications,
ck ∈ {−wl,−wl + 1, . . . ,−1, 0, 1, . . . , wu − 1, wu}. In such a case, wl is called
the lower band width and wu is called the upper band width. These patterned
matrices arise in time series and other stochastic process models as well as in
solutions of diﬀerential equations, and so they are very important in certain
applications. Although it is often the case that interesting band matrices are
symmetric, or at least have the same number of codiagonals that are nonzero,
neither of these conditions always occurs in applications of band matrices. If
all elements below the principal skew diagonal elements of a matrix are 0, the
matrix is called a skew upper triangular matrix. A common form of Hankel
matrix, for example, is the skew upper triangular matrix (see page 312). Notice
that the various terms deﬁned here, such as triangular and band, also apply
to nonsquare matrices.

Band matrices occur often in numerical solutions of partial diﬀerential
equations. A band matrix with lower and upper band widths of 1 is a tridi-
agonal matrix. If all diagonal elements and all elements ai,i±1 are nonzero, a
tridiagonal matrix is called a “matrix of type 2”. The inverse of a covariance
matrix that occurs in common stationary time series models is a matrix of
type 2 (see page 312).

Because the matrices with special patterns are usually characterized by
the locations of zeros and nonzeros, we often use an intuitive notation with X
and 0 to indicate the pattern. Thus, a band matrix may be written as

44

3 Basic Properties of Matrices

⎡⎢⎢⎢⎢⎢⎣

X X 0 ··· 0 0
X X X ··· 0 0
0 X X ··· 0 0
...
0 0 0 ··· X X

...

.

⎤⎥⎥⎥⎥⎥⎦

Computational methods for matrices may be more eﬃcient if the patterns are
taken into account.

A matrix is in upper Hessenberg form, and is called a Hessenberg matrix, if
it is upper triangular except for the ﬁrst subdiagonal, which may be nonzero.
That is, aij = 0 for i > j + 1:

⎡⎢⎢⎢⎢⎢⎢⎢⎣

X X X ··· X X
X X X ··· X X
0 X X ··· X X
0 0 X ··· X X
...
...
...
...
0 0 0 ··· X X

...

.

⎤⎥⎥⎥⎥⎥⎥⎥⎦

A symmetric matrix that is in Hessenberg form is necessarily tridiagonal.

Hessenberg matrices arise in some methods for computing eigenvalues (see

Chapter 7).

3.1.1 Matrix Shaping Operators

In order to perform certain operations on matrices and vectors, it is often
useful ﬁrst to reshape a matrix. The most common reshaping operation is
the transpose, which we deﬁne in this section. Sometimes we may need to
rearrange the elements of a matrix or form a vector into a special matrix. In
this section, we deﬁne three operators for doing this.

Transpose

The transpose of a matrix is the matrix whose ith row is the ith column of the
original matrix and whose jth column is the jth row of the original matrix. We
use a superscript “T” to denote the transpose of a matrix; thus, if A = (aij),
then

(3.2)
(In other literature, the transpose is often denoted by a prime, as in A′ =
(aji) = AT.)

AT = (aji).

If the elements of the matrix are from the ﬁeld of complex numbers, the
conjugate transpose, also called the adjoint, is more useful than the transpose.
(“Adjoint” is also used to denote another type of matrix, so we will generally
avoid using that term. This meaning of the word is the origin of the other

3.1 Basic Deﬁnitions and Notation

45

term for a Hermitian matrix, a “self-adjoint matrix”.) We use a superscript
“H” to denote the conjugate transpose of a matrix; thus, if A = (aij), then
AH = (¯aji). We also use a similar notation for vectors. If the elements of A
are all real, then AH = AT. (The conjugate transpose is often denoted by an
asterisk, as in A∗ = (¯aji) = AH. This notation is more common if a prime is
used to denote the transpose. We sometimes use the notation A∗ to denote a
g2 inverse of the matrix A; see page 102.)

If (and only if) A is symmetric, A = AT; if (and only if) A is skew sym-

metric, AT = −A; and if (and only if) A is Hermitian, A = AH.
Diagonal Matrices and Diagonal Vectors: diag(·) and vecdiag(·)
A square diagonal matrix can be speciﬁed by the diag(·) constructor function
that operates on a vector and forms a diagonal matrix with the elements of
the vector along the diagonal:

.

(3.3)

diag)(d1, d2, . . . , dn)* =⎡⎢⎢⎢⎣

d1 0 ··· 0
0 d2 ··· 0

...

0 0 ··· dn

⎤⎥⎥⎥⎦

(Notice that the argument of diag is a vector; that is why there are two sets
of parentheses in the expression above, although sometimes we omit one set
without loss of clarity.) The diag function deﬁned here is a mapping IRn $→
IRn×n. Later we will extend this deﬁnition slightly.
The vecdiag(·) function forms a vector from the principal diagonal elements

of a matrix. If A is an n × m matrix, and k = min(n, m),

vecdiag(A) = (a11, . . . , akk).

(3.4)

The vecdiag function deﬁned here is a mapping IRn×m $→ IRmin(n,m).
Sometimes we overload diag(·) to allow its argument to be a matrix, and
in that case, it is the same as vecdiag(·). The R system, for example, uses this
overloading.

Forming a Vector from the Elements of a Matrix: vec(·) and
vech(·)
It is sometimes useful to consider the elements of a matrix to be elements of
a single vector. The most common way this is done is to string the columns
of the matrix end-to-end into a vector. The vec(·) function does this:

vec(A) = (aT

1 , aT

2 , . . . , aT

m),

(3.5)

where a1, a2, . . . , am are the column vectors of the matrix A. The vec function
is also sometimes called the “pack” function. (A note on the notation: the

46

3 Basic Properties of Matrices

right side of equation (3.5) is the notation for a column vector with elements
i ; see Chapter 1.) The vec function is a mapping IRn×m $→ IRnm.
aT
For a symmetric matrix A with elements aij, the “vech” function stacks
the unique elements into a vector:

vech(A) = (a11, a21, . . . , am1, a22, . . . , am2, . . . , amm).

(3.6)

There are other ways that the unique elements could be stacked that would
be simpler and perhaps more useful (see the discussion of symmetric storage
mode on page 451), but equation (3.6) is the standard deﬁnition of vech(·).
The vech function is a mapping IRn×n $→ IRn(n+1)/2.
3.1.2 Partitioned Matrices

We often ﬁnd it useful to partition a matrix into submatrices; for example,
in many applications in data analysis, it is often convenient to work with
submatrices of various types representing diﬀerent subsets of the data.

We usually denote the submatrices with capital letters with subscripts

indicating the relative positions of the submatrices. Hence, we may write

A21 A22, ,
A =+ A11 A12

(3.7)

where the matrices A11 and A12 have the same number of rows, A21 and
A22 have the same number of rows, A11 and A21 have the same number of
columns, and A12 and A22 have the same number of columns. Of course, the
submatrices in a partitioned matrix may be denoted by diﬀerent letters. Also,
for clarity, sometimes we use a vertical bar to indicate a partition:

A = [ B | C ].

The vertical bar is used just for clarity and has no special meaning in this
representation.

The term “submatrix” is also used to refer to a matrix formed from a
given matrix by deleting various rows and columns of the given matrix. In
this terminology, B is a submatrix of A if for each element bij there is an akl
with k ≥ i and l ≥ j such that bij = akl; that is, the rows and/or columns of
the submatrix are not necessarily contiguous in the original matrix. This kind
of subsetting is often done in data analysis, for example, in variable selection
in linear regression analysis.

A square submatrix whose principal diagonal elements are elements of the
principal diagonal of the given matrix is called a principal submatrix. If A11 in
the example above is square, it is a principal submatrix, and if A22 is square,
it is also a principal submatrix. Sometimes the term “principal submatrix” is
restricted to square submatrices. If a matrix is diagonally dominant, then it
is clear that any principal submatrix of it is also diagonally dominant.

3.1 Basic Deﬁnitions and Notation

47

A principal submatrix that contains the (1, 1) elements and whose rows
and columns are contiguous in the original matrix is called a leading principal
submatrix. If A11 is square, it is a leading principal submatrix in the example
above.

Partitioned matrices may have useful patterns. A “block diagonal” matrix

is one of the form

⎡⎢⎢⎢⎣

X 0 ··· 0
0 X ··· 0

...

0 0 ··· X

,

⎤⎥⎥⎥⎦

where 0 represents a submatrix with all zeros and X represents a general
submatrix with at least some nonzeros.

The diag(·) function previously introduced for a vector is also deﬁned for

a list of matrices:

diag(A1, A2, . . . , Ak)

denotes the block diagonal matrix with submatrices A1, A2, . . . , Ak along the
diagonal and zeros elsewhere. A matrix formed in this way is sometimes called
a direct sum of A1, A2, . . . , Ak, and the operation is denoted by ⊕:

A1 ⊕ ··· ⊕ Ak = diag(A1, . . . , Ak).

Although the direct sum is a binary operation, we are justiﬁed in deﬁning

it for a list of matrices because the operation is clearly associative.

The Ai may be of diﬀerent sizes and they may not be square, although in
most applications the matrices are square (and some authors deﬁne the direct
sum only for square matrices).

We will deﬁne vector spaces of matrices below and then recall the deﬁnition
of a direct sum of vector spaces (page 13), which is diﬀerent from the direct
sum deﬁned above in terms of diag(·).
Transposes of Partitioned Matrices

The transpose of a partitioned matrix is formed in the obvious way; for ex-
ample,

+ A11 A12 A13
A21 A22 A23,T

=⎡⎣

AT
AT
AT

11 AT
21
12 AT
22
13 AT
23

⎤⎦ .

(3.8)

3.1.3 Matrix Addition

The sum of two matrices of the same shape is the matrix whose elements
are the sums of the corresponding elements of the addends. As in the case of
vector addition, we overload the usual symbols for the operations on the reals

48

3 Basic Properties of Matrices

to signify the corresponding operations on matrices when the operations are
deﬁned; hence, addition of matrices is also indicated by “+”, as with scalar
addition and vector addition. We assume throughout that writing a sum of
matrices A + B implies that they are of the same shape; that is, that they are
conformable for addition.

The “+” operator can also mean addition of a scalar to a matrix, as in
A + a, where A is a matrix and a is a scalar. Although this meaning of “+”
is generally not used in mathematical treatments of matrices, in this book
we use it to mean the addition of the scalar to each element of the matrix,
resulting in a matrix of the same shape. This meaning is consistent with the
semantics of modern computer languages such as Fortran 90/95 and R.

matrix requires nm scalar additions.

The addition of two n× m matrices or the addition of a scalar to an n× m
The matrix additive identity is a matrix with all elements zero. We some-
times denote such a matrix with n rows and m columns as 0n×m, or just as 0.
We may denote a square additive identity as 0n.
There are several possible ways to form a rank ordering of matrices of
the same shape, but no complete ordering is entirely satisfactory. If all of the
elements of the matrix A are positive, we write

if all of the elements are nonnegative, we write

A > 0;

A ≥ 0.

(3.9)

(3.10)

The terms “positive” and “nonnegative” and these symbols are not to be
confused with the terms “positive deﬁnite” and “nonnegative deﬁnite” and
similar symbols for important classes of matrices having diﬀerent properties
(which we will introduce in equation (3.62) and discuss further in Section 8.3.)

The transpose of the sum of two matrices is the sum of the transposes:

(A + B)T = AT + BT.

The sum of two symmetric matrices is therefore symmetric.

Vector Spaces of Matrices

Having deﬁned scalar multiplication, matrix addition (for conformable matri-
ces), and a matrix additive identity, we can deﬁne a vector space of n × m
matrices as any set that is closed with respect to those operations (which
necessarily would contain the additive identity; see page 11). As with any
vector space, we have the concepts of linear independence, generating set or
spanning set, basis set, essentially disjoint spaces, and direct sums of matrix
vector spaces (as in equation (2.7), which is diﬀerent from the direct sum of
matrices deﬁned in terms of diag(·)).

3.1 Basic Deﬁnitions and Notation

49

With scalar multiplication, matrix addition, and a matrix additive identity,
we see that IRn×m is a vector space. If n ≥ m, a set of nm n × m matrices
whose columns consist of all combinations of a set of n n-vectors that span IRn
is a basis set for IRn×m. If n < m, we can likewise form a basis set for IRn×m
or for subspaces of IRn×m in a similar way. If {B1, . . . , Bk} is a basis set for
IRn×m, then any n × m matrix can be represented as"k
i=1 ciBi. Subsets of a
basis set generate subspaces of IRn×m.
Because the sum of two symmetric matrices is symmetric, and a scalar
multiple of a symmetric matrix is likewise symmetric, we have a vector space
of the n × n symmetric matrices. This is clearly a subspace of the vector
space IRn×n. All vectors in any basis for this vector space must be symmetric.
Using a process similar to our development of a basis for a general vector
space of matrices, we see that there are n(n + 1)/2 matrices in the basis (see
Exercise 3.1).

3.1.4 Scalar-Valued Operators on Square Matrices:
The Trace

There are several useful mappings from matrices to real numbers; that is, from
IRn×m to IR. Some important ones are norms, which are similar to vector
norms and which we will consider later. In this section and the next, we
deﬁne two scalar-valued operators, the trace and the determinant, that apply
to square matrices.

The Trace: tr(·)
The sum of the diagonal elements of a square matrix is called the trace of the
matrix. We use the notation “tr(A)” to denote the trace of the matrix A:

tr(A) =!i

aii.

(3.11)

The Trace of the Transpose of Square Matrices

From the deﬁnition, we see

tr(A) = tr(AT).

(3.12)

The Trace of Scalar Products of Square Matrices
For a scalar c and an n × n matrix A,

tr(cA) = c tr(A).

This follows immediately from the deﬁnition because for tr(cA) each diagonal
element is multiplied by c.

50

3 Basic Properties of Matrices

The Trace of Partitioned Square Matrices

If the square matrix A is partitioned such that the diagonal blocks are square
submatrices, that is,

where A11 and A22 are square, then from the deﬁnition, we see that

A =+ A11 A12
A21 A22, ,

tr(A) = tr(A11) + tr(A22).

(3.13)

(3.14)

The Trace of the Sum of Square Matrices

If A and B are square matrices of the same order, a useful (and obvious)
property of the trace is

tr(A + B) = tr(A) + tr(B).

(3.15)

3.1.5 Scalar-Valued Operators on Square Matrices:
The Determinant

The determinant, like the trace, is a mapping from IRn×n to IR. Although
it may not be obvious from the deﬁnition below, the determinant has far-
reaching applications in matrix theory.

The Determinant: | · | or det(·)
For an n × n (square) matrix A, consider the product a1j1a2j2 ··· anjn, where
πj = (j1, j2, . . . , jn) is one of the n! permutations of the integers from 1 to n.
Deﬁne a permutation to be even or odd according to the number of times that
a smaller element follows a larger one in the permutation. (For example, 1, 3,
2 is an odd permutation, and 3, 1, 2 is an even permutation.) Let σ(πj) = 1 if
πj = (j1, . . . , jn) is an even permutation, and let σ(πj) = −1 otherwise. Then
the determinant of A, denoted by |A|, is deﬁned by

|A| = !all permutations

σ(πj)a1j1 ··· anjn.

(3.16)

The determinant is also sometimes written as det(A), especially, for exam-
ple, when we wish to refer to the absolute value of the determinant. (The
determinant of a matrix may be negative.)

The deﬁnition is not as daunting as it may appear at ﬁrst glance. Many
properties become obvious when we realize that σ(·) is always ±1, and it
can be built up by elementary exchanges of adjacent elements. For example,
consider σ(3, 2, 1). There are three elementary exchanges beginning with the
natural ordering:

3.1 Basic Deﬁnitions and Notation

51

(1, 2, 3) → (2, 1, 3) → (2, 3, 1) → (3, 2, 1);

hence, σ(3, 2, 1) = (−1)3 = −1.
If πj consists of the interchange of exactly two elements in (1, . . . , n), say
elements p and q with p < q, then there are q − p elements before p that
are larger than p, and there are q − p + 1 elements between q and p in the
permutation each with exactly one larger element preceding it. The total
number is 2q − 2p + 1, which is an odd number. Therefore, if πj consists of
the interchange of exactly two elements, then σ(πj) = −1.
If the integers 1, . . . , m and m + 1, . . . , n are together in a given permuta-
tion, they can be considered separately:

σ(j1, . . . , jn) = σ(j1, . . . , jm)σ(jm+1, . . . , jn).

(3.17)
Furthermore, we see that the product a1j1 ··· anjn has exactly one factor from
each unique row-column pair. These observations facilitate the derivation of
various properties of the determinant (although the details are sometimes
quite tedious).

We see immediately from the deﬁnition that the determinant of an upper
or lower triangular matrix (or a diagonal matrix) is merely the product of the
diagonal elements (because in each term of equation (3.16) there is a 0, except
in the term in which the subscripts on each factor are the same).

Minors, Cofactors, and Adjugate Matrices
Consider the 2 × 2 matrix

From the deﬁnition, we see |A| = a11a22 + (−1)a21a12.

Now let A be a 3 × 3 matrix:

a21 a22, .
A =+ a11 a12
A =⎡⎣
⎤⎦ .

a11 a12 a13
a21 a22 a23
a31 a32 a33

a32 a33,----
|A| = a11(−1)1+1----+ a22 a32
a31 a33,----
+ a12(−1)1+2----+ a21 a32
a31 a32,---- .
+ a13(−1)1+3----+ a21 a22

In the deﬁnition of the determinant, consider all of the terms in which the
elements of the ﬁrst row of A appear. With some manipulation of those terms,
we can express the determinant in terms of determinants of submatrices as

(3.18)

52

3 Basic Properties of Matrices

This exercise in manipulation of the terms in the determinant could be carried
out with other rows of A.

The determinants of the 2 × 2 submatrices in equation (3.18) are called
minors or complementary minors of the associated element. The deﬁnition
can be extended to (n − 1) × (n − 1) submatrices of an n × n matrix. We
denote the minor associated with the aij element as

|A−(i)(j)|,

(3.19)
in which A−(i)(j) denotes the submatrix that is formed from A by removing
the ith row and the jth column. The sign associated with the minor corre-
sponding to aij is (−1)i+j. The minor together with its appropriate sign is
called the cofactor of the associated element; that is, the cofactor of aij is
(−1)i+j|A−(i)(j)|. We denote the cofactor of aij as a(ij):

a(ij) = (−1)i+j|A−(i)(j)|.
Notice that both minors and cofactors are scalars.

(3.20)

The manipulations leading to equation (3.18), though somewhat tedious,
can be carried out for a square matrix of any size, and minors and cofactors
are deﬁned as above. An expression such as in equation (3.18) is called an
expansion in minors or an expansion in cofactors.

The extension of the expansion (3.18) to an expression involving a sum
of signed products of complementary minors arising from (n − 1) × (n − 1)
submatrices of an n × n matrix A is
n!j=1
n!j=1

aij(−1)i+j|A−(i)(j)|

|A| =

(3.21)

=

aija(ij),

or, over the rows,

|A| =

n!i=1

aija(ij).

(3.22)

These expressions are called Laplace expansions. Each determinant |A−(i)(j)|
can likewise be expressed recursively in a similar expansion.
Expressions (3.21) and (3.22) are special cases of a more general Laplace
expansion based on an extension of the concept of a complementary minor
of an element to that of a complementary minor of a minor. The derivation
of the general Laplace expansion is straightforward but rather tedious (see
Harville, 1997, for example, for the details).

Laplace expansions could be used to compute the determinant, but the
main value of these expansions is in proving properties of determinants. For
example, from the special Laplace expansion (3.21) or (3.22), we can quickly

3.1 Basic Deﬁnitions and Notation

53

see that the determinant of a matrix with two rows that are the same is zero.
We see this by recursively expanding all of the minors until we have only 2×2
matrices consisting of a duplicated row. The determinant of such a matrix is
0, so the expansion is 0.

The expansion in equation (3.21) has an interesting property: if instead of
the elements aij from the ith row we use elements from a diﬀerent row, say
the kth row, the sum is zero. That is, for k ̸= i,
n!j=1
akj(−1)i+j|A−(i)(j)| =

n!j=1

akja(ij)

= 0.

(3.23)

This is true because such an expansion is exactly the same as an expansion for
the determinant of a matrix whose kth row has been replaced by its ith row;
that is, a matrix with two identical rows. The determinant of such a matrix
is 0, as we saw above.

A certain matrix formed from the cofactors has some interesting properties.
We deﬁne the matrix here but defer further discussion. The adjugate of the
n × n matrix A is deﬁned as

adj(A) = (a(ji)),

(3.24)
which is an n × n matrix of the cofactors of the elements of the transposed
matrix. (The adjugate is also called the adjoint, but as we noted above, the
term adjoint may also mean the conjugate transpose. To distinguish it from
the conjugate transpose, the adjugate is also sometimes called the “classical
adjoint”. We will generally avoid using the term “adjoint”.) Note the reversal
of the subscripts; that is,

adj(A) = (a(ij))T.

The adjugate has an interesting property:

A adj(A) = adj(A)A = |A|I.

(3.25)

To see this, consider the (ij)T element of A adj(A), "k aik(adj(A))kj. Now,

noting the reversal of the subscripts in adj(A) in equation (3.24), and using
equations (3.21) and (3.23), we have

aik(adj(A))kj =.|A| if i = j
if i ̸= j;

0

!k
that is, A adj(A) = |A|I.
encounter later, as in equation (3.131).

The adjugate has a number of useful properties, some of which we will

54

3 Basic Properties of Matrices

The Determinant of the Transpose of Square Matrices

One important property we see immediately from a manipulation of the deﬁ-
nition of the determinant is

|A| = |AT|.

(3.26)

The Determinant of Scalar Products of Square Matrices
For a scalar c and an n × n matrix A,

(3.27)
This follows immediately from the deﬁnition because, for |cA|, each factor in
each term of equation (3.16) is multiplied by c.

|cA| = cn|A|.

The Determinant of an Upper (or Lower) Triangular Matrix
If A is an n × n upper (or lower) triangular matrix, then

|A| =

aii.

n/i=1

(3.28)

This follows immediately from the deﬁnition. It can be generalized, as in the
next section.

The Determinant of Certain Partitioned Square Matrices

Determinants of square partitioned matrices that are block diagonal or upper
or lower block triangular depend only on the diagonal partitions:

|A| =----+ A11 0

0 A22,---- =----+ A11 0

A21 A22,---- =----+ A11 A12
0 A22,----

= |A11||A22|.

(3.29)

We can see this by considering the individual terms in the determinant, equa-
tion (3.16). Suppose the full matrix is n × n, and A11 is m × m. Then A22
is (n − m) × (n − m), A21 is (n − m) × m, and A12 is m × (n − m). In
equation (3.16), any addend for which (j1, . . . , jm) is not a permutation of the
integers 1, . . . , m contains a factor aij that is in a 0 diagonal block, and hence
the addend is 0. The determinant consists only of those addends for which
(j1, . . . , jm) is a permutation of the integers 1, . . . , m, and hence (jm+1, . . . , jn)
is a permutation of the integers m + 1, . . . , n,

|A| =!! σ(j1, . . . , jm, jm+1, . . . , jn)a1j1 ··· amjmam+1,jn ··· anjn,

3.1 Basic Deﬁnitions and Notation

55

where the ﬁrst sum is taken over all permutations that keep the ﬁrst m integers
together while maintaining a ﬁxed ordering for the integers m + 1 through n,
and the second sum is taken over all permutations of the integers from m + 1
through n while maintaining a ﬁxed ordering of the integers from 1 to m.
Now, using equation (3.17), we therefore have for A of this special form

|A| =!! σ(j1, . . . , jm, jm+1, . . . , jn)a1j1 ··· amjmam+1,jm+1 ··· anjn
=! σ(j1, . . . , jm)a1j1 ··· amjm! σ(jm+1, . . . , jn)am+1,jm+1 ··· anjn
= |A11||A22|,

which is equation (3.29). We use this result to give an expression for the
determinant of more general partitioned matrices in Section 3.4.2.

Another useful partitioned matrix of the form of equation (3.13) has A11 =

0 and A21 = −I:

A =+ 0 A12
−I A22, .

In this case, using equation (3.21), we get

|A| = ((−1)n+1+1(−1))n|A12|

= (−1)n(n+3)|A12|
= |A12|.

(3.30)

The Determinant of the Sum of Square Matrices

Occasionally it is of interest to consider the determinant of the sum of square
matrices. We note in general that

|A + B| ̸= |A| + |B|,

which we can see easily by an example. (Consider matrices in IR2×2, for ex-

ample, and let A = I and B =+−1 0
0 0,.)

In some cases, however, simpliﬁed expressions for the determinant of a

sum can be developed. We consider one in the next section.

A Diagonal Expansion of the Determinant

A particular sum of matrices whose determinant is of interest is one in which
a diagonal matrix D is added to a square matrix A, that is, |A + D|. (Such a
determinant arises in eigenanalysis, for example, as we see in Section 3.8.2.)
For evaluating the determinant |A + D|, we can develop another expan-
sion of the determinant by restricting our choice of minors to determinants of
matrices formed by deleting the same rows and columns and then continuing

56

3 Basic Properties of Matrices

to delete rows and columns recursively from the resulting matrices. The ex-
pansion is a polynomial in the elements of D; and for our purposes later, that
is the most useful form.

Before considering the details, let us develop some additional notation.
The matrix formed by deleting the same row and column of A is denoted
A−(i)(i) as above (following equation (3.19)). In the current context, however,
it is more convenient to adopt the notation A(i1,...,ik) to represent the matrix
formed from rows i1, . . . , ik and columns i1, . . . , ik from a given matrix A.
That is, the notation A(i1,...,ik) indicates the rows and columns kept rather
than those deleted; and furthermore, in this notation, the indexes of the rows
and columns are the same. We denote the determinant of this k × k matrix in
the obvious way, |A(i1,...,ik)|. Because the principal diagonal elements of this
matrix are principal diagonal elements of A, we call |A(i1,...,ik)| a principal
minor of A.

Expanding this, we have

Now consider |A + D| for the 2 × 2 case:
a12

a21

|A + D| = (a11 + d1)(a22 + d2) − a12a21

a22 + d2,---- .

----+ a11 + d1
=----+ a11 a12
a21 a22,---- + d1d2 + a22d1 + a11d2

= |A(1,2)| + d1d2 + a22d1 + a11d2.

Of course, |A(1,2)| = |A|, but we are writing it this way to develop the pattern.
Now, for the 3 × 3 case, we have
|A + D| = |A(1,2,3)|

+|A(2,3)|d1 + |A(1,3)|d2 + |A(1,2)|d3
+ a33d1d2 + a22d1d3 + a11d2d3
+ d1d2d3.

(3.31)

In the applications of interest, the elements of the diagonal matrix D may be
a single variable: d, say. In this case, the expression simpliﬁes to

|A + D| = |A(1,2,3)| +!i̸=j

|A(i,j)|d +!i

ai,id2 + d3.

(3.32)

Carefully continuing in this way for an n×n matrix, either as in equation (3.31)
for n variables or as in equation (3.32) for a single variable, we can make use
of a Laplace expansion to evaluate the determinant.

3.1 Basic Deﬁnitions and Notation

57

Consider the expansion in a single variable because that will prove most
useful. The pattern persists; the constant term is |A|, the coeﬃcient of the
ﬁrst-degree term is the sum of the (n − 1)-order principal minors, and, at
the other end, the coeﬃcient of the (n − 1)th-degree term is the sum of the
ﬁrst-order principal minors (that is, just the diagonal elements), and ﬁnally
the coeﬃcient of the nth-degree term is 1.

This kind of representation is called a diagonal expansion of the determi-
nant because the coeﬃcients are principal minors. It has occasional use for
matrices with large patterns of zeros, but its main application is in analysis
of eigenvalues, which we consider in Section 3.8.2.

Computing the Determinant

For an arbitrary matrix, the determinant is rather diﬃcult to compute. The
method for computing a determinant is not the one that would arise directly
from the deﬁnition or even from a Laplace expansion. The more eﬃcient meth-
ods involve ﬁrst factoring the matrix, as we discuss in later sections.

The determinant is not very often directly useful, but although it may
not be obvious from its deﬁnition, the determinant, along with minors, co-
factors, and adjoint matrices, is very useful in discovering and proving prop-
erties of matrices. The determinant is used extensively in eigenanalysis (see
Section 3.8).

A Geometrical Perspective of the Determinant

In Section 2.2, we discussed a useful geometric interpretation of vectors in
a linear space with a Cartesian coordinate system. The elements of a vec-
tor correspond to measurements along the respective axes of the coordinate
system. When working with several vectors, or with a matrix in which the
columns (or rows) are associated with vectors, we may designate a vector
xi as xi = (xi1, . . . , xid). A set of d linearly independent d-vectors deﬁne a
parallelotope in d dimensions. For example, in a two-dimensional space, the
linearly independent 2-vectors x1 and x2 deﬁne a parallelogram, as shown in
Figure 3.1.

The area of this parallelogram is the base times the height, bh, where, in
this case, b is the length of the vector x1, and h is the length of x2 times the
sine of the angle θ. Thus, making use of equation (2.32) on page 26 for the
cosine of the angle, we have

58

3 Basic Properties of Matrices

e2

x2

a

θ

x1

h

b

e1

Fig. 3.1. Volume (Area) of Region Determined by x1 and x2

area = bh

= ∥x1∥∥x2∥ sin(θ)

= ∥x1∥∥x2∥01 −1 ⟨x1, x2⟩
∥x1∥∥x2∥22
=3∥x1∥2∥x2∥2 − (⟨x1, x2⟩)2
=4(x2
22) − (x11x21 − x12x22)2
12)(x2
= |x11x22 − x12x21|
= |det(X)|,

11 + x2

21 + x2

(3.33)

where x1 = (x11, x12), x2 = (x21, x22), and
X = [x1 | x2]
=+ x11 x21
x12 x22, .

Although we will not go through the details here, this equivalence of a
volume of a parallelotope that has a vertex at the origin and the absolute
value of the determinant of a square matrix whose columns correspond to the
vectors that form the sides of the parallelotope extends to higher dimensions.
In making a change of variables in integrals, as in equation (4.37) on
page 165, we use the absolute value of the determinant of the Jacobian as a
volume element. Another instance of the interpretation of the determinant as
a volume is in the generalized variance, discussed on page 296.

3.2 Multiplication of Matrices

59

3.2 Multiplication of Matrices and
Multiplication of Vectors and Matrices

The elements of a vector or matrix are elements of a ﬁeld, and most matrix
and vector operations are deﬁned in terms of the two operations of the ﬁeld.
Of course, in this book, the ﬁeld of most interest is the ﬁeld of real numbers.

3.2.1 Matrix Multiplication (Cayley)

There are various kinds of multiplication of matrices that may be useful. The
most common kind of multiplication is Cayley multiplication. If the number
of columns of the matrix A, with elements aij, and the number of rows of the
matrix B, with elements bij, are equal, then the (Cayley) product of A and B
is deﬁned as the matrix C with elements

cij =!k

aikbkj.

(3.34)

This is the most common type of matrix product, and we refer to it by the
unqualiﬁed phrase “matrix multiplication”.

Cayley matrix multiplication is indicated by juxtaposition, with no inter-

vening symbol for the operation: C = AB.

If the matrix A is n × m and the matrix B is m × p, the product C = AB

is n × p:

C

= A

B

+

,n×p

=+ ,n×m

[

Cayley matrix multiplication is a mapping,

.

]m×p

IRn×m × IRm×p $→ IRn×p.

The multiplication of an n × m matrix and an m × p matrix requires
nmp scalar multiplications and np(m − 1) scalar additions. Here, as always
in numerical analysis, we must remember that the deﬁnition of an operation,
such as matrix multiplication, does not necessarily deﬁne a good algorithm
for evaluating the operation.

It is obvious that while the product AB may be well-deﬁned, the product
BA is deﬁned only if n = p; that is, if the matrices AB and BA are square.
We assume throughout that writing a product of matrices AB implies that
the number of columns of the ﬁrst matrix is the same as the number of rows of
the second; that is, they are conformable for multiplication in the order given.
It is easy to see from the deﬁnition of matrix multiplication (3.34) that
in general, even for square matrices, AB ̸= BA. It is also obvious that if AB
exists, then BTAT exists and, in fact,

60

3 Basic Properties of Matrices

(3.35)
The product of symmetric matrices is not, in general, symmetric. If (but not
only if) A and B are symmetric, then AB = (BA)T.

BTAT = (AB)T.

Various matrix shapes are preserved under matrix multiplication. Assume
A and B are square matrices of the same number of rows. If A and B are
diagonal, AB is diagonal; if A and B are upper triangular, AB is upper
triangular; and if A and B are lower triangular, AB is lower triangular.

Because matrix multiplication is not commutative, we often use the terms
“premultiply” and “postmultiply” and the corresponding nominal forms of
these terms. Thus, in the product AB, we may say B is premultiplied by A,
or, equivalently, A is postmultiplied by B.

Although matrix multiplication is not commutative, it is associative; that

is, if the matrices are conformable,

It is also distributive over addition; that is,

A(BC) = (AB)C.

A(B + C) = AB + AC

(3.36)

(3.37)

and

(B + C)A = BA + CA.

(3.38)
These properties are obvious from the deﬁnition of matrix multiplication.
(Note that left-sided distribution is not the same as right-sided distribution
because the multiplication is not commutative.)

An n×n matrix consisting of 1s along the diagonal and 0s everywhere else is
a multiplicative identity for the set of n×n matrices and Cayley multiplication.
Such a matrix is called the identity matrix of order n, and is denoted by In,
or just by I. The columns of the identity matrix are unit vectors.

The identity matrix is a multiplicative identity for any matrix so long as

the matrices are conformable for the multiplication. If A is n × m, then

InA = AIm = A.

Powers of Square Matrices
For a square matrix A, its product with itself is deﬁned, and so we will use the
notation A2 to mean the Cayley product AA, with similar meanings for Ak
for a positive integer k. As with the analogous scalar case, Ak for a negative
integer may or may not exist, and when it exists, it has a meaning for Cayley
multiplication similar to the meaning in ordinary scalar multiplication. We
will consider these issues later (in Section 3.3.3).

For an n × n matrix A, if Ak exists for negative integers, we deﬁne A0 by
(3.39)

A0 = In.

For a diagonal matrix D = diag ((d1, . . . , dn)), we have

Dk = diag)(dk

1, . . . , dk

n)* .

(3.40)

3.2 Multiplication of Matrices

61

Matrix Polynomials

Polynomials in square matrices are similar to the more familiar polynomials
in scalars. We may consider

p(A) = b0I + b1A + ··· bkAk.

The value of this polynomial is a matrix.

The theory of polynomials in general holds, and in particular, we have the

useful factorizations of monomials: for any positive integer k,

I − Ak = (I − A)(I + A + ··· Ak−1),

and for an odd positive integer k,

I + Ak = (I + A)(I − A + ··· Ak−1).

3.2.2 Multiplication of Partitioned Matrices

(3.41)

(3.42)

Multiplication and other operations with partitioned matrices are carried out
with their submatrices in the obvious way. Thus, assuming the submatrices
are conformable for multiplication,

A21 A22,+ B11 B12
+ A11 A12

B21 B22, =+ A11B11 + A12B21 A11B12 + A12B22
A21B11 + A22B21 A21B12 + A22B22, .

Sometimes a matrix may be partitioned such that one partition is just a
single column or row, that is, a vector or the transpose of a vector. In that
case, we may use a notation such as

[X y]

or

[X | y],

where X is a matrix and y is a vector. We develop the notation in the obvious
fashion; for example,

[X y]T [X y] =+ X TX X Ty
yTX yTy, .

(3.43)

3.2.3 Elementary Operations on Matrices

Many common computations involving matrices can be performed as a se-
quence of three simple types of operations on either the rows or the columns
of the matrix:
•

the interchange of two rows (columns),

62

3 Basic Properties of Matrices

•
•

a scalar multiplication of a given row (column), and
the replacement of a given row (column) by the sum of that row
(columns) and a scalar multiple of another row (column); that is, an
axpy operation.

Such an operation on the rows of a matrix can be performed by premultipli-
cation by a matrix in a standard form, and an operation on the columns of
a matrix can be performed by postmultiplication by a matrix in a standard
form. To repeat:
•
•

premultiplication: operation on rows;
postmultiplication: operation on columns.

The matrix used to perform the operation is called an elementary trans-
formation matrix or elementary operator matrix. Such a matrix is the identity
matrix transformed by the corresponding operation performed on its unit
rows, eT

p , or columns, ep.

In actual computations, we do not form the elementary transformation
matrices explicitly, but their formulation allows us to discuss the operations
in a systematic way and better understand the properties of the operations.
Products of any of these elementary operator matrices can be used to eﬀect
more complicated transformations.

Operations on the rows are more common, and that is what we will dis-
cuss here, although operations on columns are completely analogous. These
transformations of rows are called elementary row operations.

Interchange of Rows or Columns; Permutation Matrices

By ﬁrst interchanging the rows or columns of a matrix, it may be possible
to partition the matrix in such a way that the partitions have interesting
or desirable properties. Also, in the course of performing computations on a
matrix, it is often desirable to interchange the rows or columns of the matrix.
(This is an instance of “pivoting”, which will be discussed later, especially
in Chapter 6.) In matrix computations, we almost never actually move data
from one row or column to another; rather, the interchanges are eﬀected by
changing the indexes to the data.

Interchanging two rows of a matrix can be accomplished by premultiply-
ing the matrix by a matrix that is the identity with those same two rows
interchanged; for example,

1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1

⎡⎢⎢⎣

⎤⎥⎥⎦

⎡⎢⎢⎣

a11 a12 a13
a21 a22 a23
a31 a32 a33
a41 a42 a43

The ﬁrst matrix in the expression above is called an elementary permutation
matrix. It is the identity matrix with its second and third rows (or columns)

⎤⎥⎥⎦ =⎡⎢⎢⎣

a11 a12 a13
a31 a32 a33
a21 a22 a23
a41 a42 a43

⎤⎥⎥⎦ .

3.2 Multiplication of Matrices

63

interchanged. An elementary permutation matrix, which is the identity with
the pth and qth rows interchanged, is denoted by Epq. That is, Epq is the
identity, except the pth row is eT
p . Note that Epq = Eqp.
Thus, for example, if the given matrix is 4× m, to interchange the second and
third rows, we use

q and the qth row is eT

E23 = E32 =⎡⎢⎢⎣

1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1

⎤⎥⎥⎦ .

It is easy to see from the deﬁnition that an elementary permutation matrix
is symmetric. Note that the notation Epq does not indicate the order of the
elementary permutation matrix; that must be speciﬁed in the context.

Premultiplying a matrix A by a (conformable) Epq results in an inter-
change of the pth and qth rows of A as we see above. Any permutation of rows
of A can be accomplished by successive premultiplications by elementary per-
mutation matrices. Note that the order of multiplication matters. Although
a given permutation can be accomplished by diﬀerent elementary permuta-
tions, the number of elementary permutations that eﬀect a given permutation
is always either even or odd; that is, if an odd number of elementary per-
mutations results in a given permutation, any other sequence of elementary
permutations to yield the given permutation is also odd in number. Any given
permutation can be eﬀected by successive interchanges of adjacent rows.

Postmultiplying a matrix A by a (conformable) Epq results in an inter-

change of the pth and qth columns of A:

a11 a12 a13
a21 a22 a23
a31 a32 a33
a41 a42 a43

⎡⎢⎢⎣

⎤⎥⎥⎦

⎡⎣

1 0 0
0 0 1

0 1 0⎤⎦ =⎡⎢⎢⎣

a11 a13 a12
a21 a23 a22
a31 a33 a32
a41 a43 a42

⎤⎥⎥⎦ .

Note that

(3.44)
that is, as an operator, an elementary permutation matrix is its own inverse
operator: EpqEpq = I.

A = EpqEpqA = AEpqEpq;

Because all of the elements of a permutation matrix are 0 or 1, the trace

of an n × n elementary permutation matrix is n − 2.
The product of elementary permutation matrices is also a permutation
matrix in the sense that it permutes several rows or columns. For example,
premultiplying A by the matrix Q = EpqEqr will yield a matrix whose pth row
is the rth row of the original A, whose qth row is the pth row of A, and whose
rth row is the qth row of A. We often use the notation Eπ to denote a more
general permutation matrix. This expression will usually be used generically,
but sometimes we will specify the permutation, π.

64

3 Basic Properties of Matrices

A general permutation matrix (that is, a product of elementary permuta-
tion matrices) is not necessarily symmetric, but its transpose is also a per-
mutation matrix. It is not necessarily its own inverse, but its permutations
can be reversed by a permutation matrix formed by products of elementary
permutation matrices in the opposite order; that is,

ET

π Eπ = I.

As a prelude to other matrix operations, we often permute both rows and

columns, so we often have a representation such as

B = Eπ1AEπ2,

(3.45)

where Eπ1 is a permutation matrix to permute the rows and Eπ2 is a permu-
tation matrix to permute the columns. We use these kinds of operations to
arrive at the important equation (3.99) on page 80, and combine these oper-
ations with others to yield equation (3.113) on page 86. These equations are
used to determine the number of linearly independent rows and columns and
to represent the matrix in a form with a maximal set of linearly independent
rows and columns clearly identiﬁed.

The Vec-Permutation Matrix

A special permutation matrix is the matrix that transforms the vector vec(A)
into vec(AT). If A is n × m, the matrix Knm that does this is nm × nm. We
have
(3.46)

vec(AT) = Knmvec(A).

The matrix Knm is called the nm vec-permutation matrix.

Scalar Row or Column Multiplication

Often, numerical computations with matrices are more accurate if the rows
have roughly equal norms. For this and other reasons, we often transform a
matrix by multiplying one of its rows by a scalar. This transformation can also
be performed by premultiplication by an elementary transformation matrix.
For multiplication of the pth row by the scalar, the elementary transformation
matrix, which is denoted by Ep(a), is the identity matrix in which the pth
diagonal element has been replaced by a. Thus, for example, if the given
matrix is 4 × m, to multiply the second row by a, we use

E2(a) =⎡⎢⎢⎣

1 0 0 0
0 a 0 0
0 0 1 0
0 0 0 1

⎤⎥⎥⎦ .

3.2 Multiplication of Matrices

65

Postmultiplication of a given matrix by the multiplier matrix Ep(a) results
in the multiplication of the pth column by the scalar. For this, Ep(a) is a square
matrix of order equal to the number of columns of the given matrix.

Note that the notation Ep(a) does not indicate the number of rows and

columns. This must be speciﬁed in the context.

Note that, if a ̸= 0,

(3.47)
that is, as an operator, the inverse operator is a row multiplication matrix on
the same row and with the reciprocal as the multiplier.

A = Ep(1/a)Ep(a)A,

Axpy Row or Column Transformations

The other elementary operation is an axpy on two rows and a replacement of
one of those rows with the result

ap ← aaq + ap.

This operation also can be eﬀected by premultiplication by a matrix formed
from the identity matrix by inserting the scalar in the (p, q) position. Such a
matrix is denoted by Epq(a). Thus, for example, if the given matrix is 4 × m,
to add a times the third row to the second row, we use

E23(a) =⎡⎢⎢⎣

1 0 0 0
0 1 a 0
0 0 1 0
0 0 0 1

⎤⎥⎥⎦ .

Premultiplication of a matrix A by such a matrix,

Epq(a)A,

(3.48)

yields a matrix whose pth row is a times the qth row plus the original row.

Postmultiplication of a matrix A by an axpy operator matrix,

AEpq(a),

yields a matrix whose qth column is a times the pth column plus the original
column. For this, Epq(a) is a square matrix of order equal to the number of
columns of the given matrix. Note that the column that is changed corresponds
to the second subscript in Epq(a).

Given the 4 × 3 matrix A = (aij), we have
a12

a11

E23(a)A =⎡⎢⎢⎣

a13

a21 + aa31 a22 + aa32 a23 + aa33

a31
a41

a32
a42

a33
a43

⎤⎥⎥⎦ .

66

3 Basic Properties of Matrices

Note that

(3.49)
that is, as an operator, the inverse operator is the same axpy elementary
operator matrix with the negative of the multiplier.

A = Epq(−a)Epq(a)A;

A common use of axpy operator matrices is to form a matrix with zeros
in all positions of a given column below a given position in the column. These
operations usually follow an operation by a scalar row multiplier matrix that
puts a 1 in the position of interest. For example, given an n × m matrix A
with aij ̸= 0, to put a 1 in the (i, j) position and 0s in all positions of the jth
column below the ith row, we form the product

Emi(−amj)··· Ei+1,i(−ai+1,j)Ei(1/aij)A.

(3.50)

This process is called Gaussian elimination.

Gaussian elimination is often performed sequentially down the diagonal
elements of a matrix. If at some point aii = 0, the operations of equation (3.50)
cannot be performed. In that case, we may ﬁrst interchange the ith row with
the kth row, where k > i and aki ̸= 0. Such an interchange is called pivoting.
We will discuss pivoting in more detail on page 209 in Chapter 6.
To form a matrix with zeros in all positions of a given column except one,

we use additional matrices for the rows above the given element:
Emi(−amj)··· Ei+1,i(−ai+1,j)··· Ei−1,i(−ai−1,j)··· E1i(−a1j)Ei(1/aij)A.
We can likewise zero out all elements in the ith row except the one in the

(ij)th position by similar postmultiplications.

These elementary transformations are the basic operations in Gaussian

elimination, which is discussed in Sections 5.6 and 6.2.1.

Determinants of Elementary Operator Matrices

The determinant of an elementary permutation matrix Epq has only one term
in the sum that deﬁnes the determinant (equation (3.16), page 50), and that
term is 1 times σ evaluated at the permutation that exchanges p and q. As
we have seen (page 51), this is an odd permutation; hence, for an elementary
permutation matrix Epq,

(3.51)
Because all terms in |EpqA| are exactly the same terms as in |A| but with

|Epq| = −1.

one diﬀerent permutation in each term, we have
|EpqA| = −|A|.

More generally, if A and Eπ are n × n matrices, and Eπ is any permutation
matrix (that is, any product of Epq matrices), then |EπA| is either |A| or
−|A| because all terms in |EπA| are exactly the same as the terms in |A| but

3.2 Multiplication of Matrices

67

possibly with diﬀerent signs because the permutations are diﬀerent. In fact,
the diﬀerences in the permutations are exactly the same as the permutation
of 1, . . . , n in Eπ; hence,

|EπA| = |Eπ||A|.

(In equation (3.57) below, we will see that this equation holds more generally.)

The determinant of an elementary row multiplication matrix Ep(a) is

|Ep(a)| = a.
If A and Ep(a) are n × n matrices, then

|Ep(a)A| = a|A|,

as we see from the deﬁnition of the determinant, equation (3.16).

The determinant of an elementary axpy matrix Epq(a) is 1,

(3.52)

(3.53)
because the term consisting of the product of the diagonals is the only term
in the determinant.

|Epq(a)| = 1,

Now consider |Epq(a)A| for an n × n matrix A. Expansion in the minors

(equation (3.21)) along the pth row yields

|Epq(a)A| =

=

n!j=1
n!j=1

(apj + aaqj)(−1)p+j|A(ij)|
n!j=1

apj(−1)p+j|A(ij)| + a

aqj(−1)p+j|A(ij)|.

From equation (3.23) on page 53, we see that the second term is 0, and since
the ﬁrst term is just the determinant of A, we have

|Epq(a)A| = |A|.
3.2.4 Traces and Determinants of Square
Cayley Products

(3.54)

The Trace
A useful property of the trace for the matrices A and B that are conformable
for the multiplications AB and BA is

tr(AB) = tr(BA).

(3.55)

This is obvious from the deﬁnitions of matrix multiplication and the trace.

Because of the associativity of matrix multiplication, this relation can be

extended as

(3.56)
for matrices A, B, and C that are conformable for the multiplications indi-
cated. Notice that the individual matrices need not be square.

tr(ABC) = tr(BCA) = tr(CAB)

68

3 Basic Properties of Matrices

The Determinant

An important property of the determinant is
|AB| = |A||B|

if A and B are square matrices conformable for multiplication. We see this by
ﬁrst forming

(3.57)

----+ I A
0 I,+ A 0

−I B ,----
−I B,---- =----+ 0 AB

(3.58)
and then observing from equation (3.30) that the right-hand side is |AB|. Now
consider the left-hand side. The matrix that is the ﬁrst factor is a product
of elementary axpy transformation matrices; that is, it is a matrix that when
postmultiplied by another matrix merely adds multiples of rows in the lower
part of the matrix to rows in the upper part of the matrix. If A and B are
n× n (and so the identities are likewise n× n), the full matrix is the product:
+ I A
0 I, = E1,n+1(a11)··· E1,2n(a1n)E2,n+1(a21)··· E2,2n(a2,n)··· En,2n(ann).

Hence, applying equation (3.54) recursively, we have

−I B,---- ,
−I B,---- =----+ A 0
----+ I A
0 I,+ A 0
−I B,---- = |A||B|,
----+ A 0

and from equation (3.29) we have

and so ﬁnally we have equation (3.57).

3.2.5 Multiplication of Matrices and Vectors

It is often convenient to think of a vector as a matrix with only one element
in one of its dimensions. This provides for an immediate extension of the de-
ﬁnitions of transpose and matrix multiplication to include vectors as either
or both factors. In this scheme, we follow the convention that a vector corre-
sponds to a column; that is, if x is a vector and A is a matrix, Ax or xTA may
be well-deﬁned, but neither xA nor AxT would represent anything, except in
the case when all dimensions are 1. (In some computer systems for matrix
algebra, these conventions are not enforced; see, for example the R code in
Figure 12.4 on page 468.) The alternative notation xTy we introduced earlier
for the dot product or inner product, ⟨x, y⟩, of the vectors x and y is consis-
tent with this paradigm. We will continue to write vectors as x = (x1, . . . , xn),
however. This does not imply that the vector is a row vector. We would repre-
sent a matrix with one row as Y = [y11 . . . y1n] and a matrix with one column
as Z = [z11 . . . zm1]T.

3.2 Multiplication of Matrices

69

The Matrix/Vector Product as a Linear Combination
If we represent the vectors formed by the columns of an n × m matrix A
as a1, . . . , am, the matrix/vector product Ax is a linear combination of these
columns of A:

Ax =

xiai.

(3.59)

m!i=1

(Here, each xi is a scalar, and each ai is a vector.)

Given the equation Ax = b, we have b ∈ span(A); that is, the n-vector b

is in the k-dimensional column space of A, where k ≤ m.
3.2.6 Outer Products

The outer product of the vectors x and y is the matrix

xyT.

(3.60)
Note that the deﬁnition of the outer product does not require the vectors to
be of equal length. Note also that while the inner product is commutative,
the outer product is not commutative (although it does have the property
xyT = (yxT)T).

A very common outer product is of a vector with itself:

xxT.

The outer product of a vector with itself is obviously a symmetric matrix.

We should again note some subtleties of diﬀerences in the types of objects
that result from operations. If A and B are matrices conformable for the
operation, the product ATB is a matrix even if both A and B are n × 1 and
so the result is 1×1. For the vectors x and y and matrix C, however, xTy and
xTCy are scalars; hence, the dot product and a quadratic form are not the
same as the result of a matrix multiplication. The dot product is a scalar, and
the result of a matrix multiplication is a matrix. The outer product of vectors
is a matrix, even if both vectors have only one element. Nevertheless, as we
have mentioned before, in the following, we will treat a one by one matrix or
a vector with only one element as a scalar whenever it is convenient to do so.

3.2.7 Bilinear and Quadratic Forms; Deﬁniteness

A variation of the vector dot product, xTAy, is called a bilinear form, and
the special bilinear form xTAx is called a quadratic form. Although in the
deﬁnition of quadratic form we do not require A to be symmetric — because
for a given value of x and a given value of the quadratic form xTAx there is a
unique symmetric matrix As such that xTAsx = xTAx — we generally work
only with symmetric matrices in dealing with quadratic forms. (The matrix
As is 1
2(A + AT); see Exercise 3.3.) Quadratic forms correspond to sums of
squares and hence play an important role in statistical applications.

70

3 Basic Properties of Matrices

Nonnegative Deﬁnite and Positive Deﬁnite Matrices

A symmetric matrix A such that for any (conformable and real) vector x the
quadratic form xTAx is nonnegative, that is,
xTAx ≥ 0,

(3.61)

is called a nonnegative deﬁnite matrix. We denote the fact that A is nonneg-
ative deﬁnite by

(Note that we consider 0n×n to be nonnegative deﬁnite.)
quadratic form

A symmetric matrix A such that for any (conformable) vector x ̸= 0 the
(3.62)
is called a positive deﬁnite matrix. We denote the fact that A is positive
deﬁnite by

xTAx > 0

A ≽ 0.

A ≻ 0.

(Recall that A ≥ 0 and A > 0 mean, respectively, that all elements of A are
nonnegative and positive.) When A and B are symmetric matrices of the same
order, we write A ≽ B to mean A − B ≽ 0 and A ≻ B to mean A − B ≻ 0.
Nonnegative and positive deﬁnite matrices are very important in applications.
We will encounter them from time to time in this chapter, and then we will
discuss more of their properties in Section 8.3.

In this book we use the terms “nonnegative deﬁnite” and “positive deﬁ-
nite” only for symmetric matrices. In other literature, these terms may be used
more generally; that is, for any (square) matrix that satisﬁes (3.61) or (3.62).

The Trace of Inner and Outer Products

The invariance of the trace to permutations of the factors in a product (equa-
tion (3.55)) is particularly useful in working with quadratic forms. Because
the quadratic form itself is a scalar (or a 1 × 1 matrix), and because of the
invariance, we have the very useful fact

xTAx = tr(xTAx)
= tr(AxxT).

(3.63)

Furthermore, for any scalar a, n-vector x, and n × n matrix A, we have

(x − a)TA(x − a) = tr(AxcxT

c ) + n(a − ¯x)2tr(A).

(3.64)

(Compare this with equation (2.48) on page 34.)

3.2.8 Anisometric Spaces

3.2 Multiplication of Matrices

71

In Section 2.1, we considered various properties of vectors that depend on
the inner product, such as orthogonality of two vectors, norms of a vector,
angles between two vectors, and distances between two vectors. All of these
properties and measures are invariant to the orientation of the vectors; the
space is isometric with respect to a Cartesian coordinate system. Noting that
the inner product is the bilinear form xTIy, we have a heuristic generalization
to an anisometric space. Suppose, for example, that the scales of the coordi-
nates diﬀer; say, a given distance along one axis in the natural units of the
axis is equivalent (in some sense depending on the application) to twice that
distance along another axis, again measured in the natural units of the axis.
The properties derived from the inner product, such as a norm and a metric,
may correspond to the application better if we use a bilinear form in which
the matrix reﬂects the diﬀerent eﬀective distances along the coordinate axes.
A diagonal matrix whose entries have relative values corresponding to the
inverses of the relative scales of the axes may be more useful. Instead of xTy,
we may use xTDy, where D is this diagonal matrix.

Rather than diﬀerences in scales being just in the directions of the co-
ordinate axes, more generally we may think of anisometries being measured
by general (but perhaps symmetric) matrices. (The covariance and correlation
matrices deﬁned on page 294 come to mind. Any such matrix to be used in this
context should be positive deﬁnite because we will generalize the dot prod-
uct, which is necessarily nonnegative, in terms of a quadratic form.) A bilinear
form xTAy may correspond more closely to the properties of the application
than the standard inner product.

We deﬁne orthogonality of two vectors x and y with respect to A by

xTAy = 0.

(3.65)

In this case, we say x and y are A-conjugate.

The L2 norm of a vector is the square root of the quadratic form of the
vector with respect to the identity matrix. A generalization of the L2 vector
norm, called an elliptic norm or a conjugate norm, is deﬁned for the vector
x as the square root of the quadratic form xTAx for any symmetric positive
deﬁnite matrix A. It is sometimes denoted by ∥x∥A:

∥x∥A = √xTAx.

(3.66)

It is easy to see that

∥x∥A

satisﬁes the deﬁnition of a norm given on page 16. If A is a diagonal matrix
with elements wi ≥ 0, the elliptic norm is the weighted L2 norm of equa-
tion (2.15).
The elliptic norm yields an elliptic metric in the usual way of deﬁning a
metric in terms of a norm. The distance between the vectors x and y with

72

3 Basic Properties of Matrices

deﬁnition of a metric given on page 22.

respect to A is 3(x − y)TA(x − y). It is easy to see that this satisﬁes the

A metric that is widely useful in statistical applications is the Mahalanobis
distance, which uses a covariance matrix as the scale for a given space. (The
sample covariance matrix is deﬁned in equation (8.70) on page 294.) If S is
the covariance matrix, the Mahalanobis distance, with respect to that matrix,
between the vectors x and y is

4(x − y)TS−1(x − y).

(3.67)

3.2.9 Other Kinds of Matrix Multiplication

The most common kind of product of two matrices is the Cayley product,
and when we speak of matrix multiplication without qualiﬁcation, we mean
the Cayley product. Three other types of matrix multiplication that are use-
ful are Hadamard multiplication, Kronecker multiplication, and dot product
multiplication.

The Hadamard Product

Hadamard multiplication is deﬁned for matrices of the same shape as the mul-
tiplication of each element of one matrix by the corresponding element of the
other matrix. Hadamard multiplication immediately inherits the commutativ-
ity, associativity, and distribution over addition of the ordinary multiplication
of the underlying ﬁeld of scalars. Hadamard multiplication is also called array
multiplication and element-wise multiplication. Hadamard matrix multiplica-
tion is a mapping

IRn×m × IRn×m $→ IRn×m.

The identity for Hadamard multiplication is the matrix of appropriate

shape whose elements are all 1s.

The Kronecker Product
Kronecker multiplication, denoted by ⊗, is deﬁned for any two matrices An×m
and Bp×q as

The Kronecker product of A and B is np × mq; that is, Kronecker matrix
multiplication is a mapping

A ⊗ B =⎡⎢⎣

a11B . . . a1mB
...
an1B . . . anmB

. . .

...

⎤⎥⎦ .

IRn×m × IRp×q $→ IRnp×mq.

3.2 Multiplication of Matrices

73

The Kronecker product is also called the “right direct product” or just
direct product. (A left direct product is a Kronecker product with the factors
reversed.)

Kronecker multiplication is not commutative, but it is associative and it

is distributive over addition, as we will see below.

element 1; that is, it is the same as the scalar 1.

The identity for Kronecker multiplication is the 1 × 1 matrix with the
The determinant of the Kronecker product of two square matrices An×n
and Bm×m has a simple relationship to the determinants of the individual
matrices:
(3.68)
The proof of this, like many facts about determinants, is straightforward but
involves tedious manipulation of cofactors. The manipulations in this case can
be facilitated by using the vec-permutation matrix. See Harville (1997) for a
detailed formal proof.

|A ⊗ B| = |A|m|B|n.

We can understand the properties of the Kronecker product by expressing

the (i, j) element of A ⊗ B in terms of the elements of A and B,

(A ⊗ B)i,j = A[i/p]+1, [j/q]+1Bi−p[i/p], j−q[i/q],

(3.69)

where [·] is the greatest integer function.
Some additional properties of Kronecker products that are immediate re-
sults of the deﬁnition are, assuming the matrices are conformable for the
indicated operations,

(aA) ⊗ (bB) = ab(A ⊗ B)
= (abA) ⊗ B
= A ⊗ (abB), for scalars a, b,

(A + B) ⊗ (C) = A ⊗ C + B ⊗ C,

(A ⊗ B) ⊗ C = A ⊗ (B ⊗ C),

(A ⊗ B)T = AT ⊗ BT,

(A ⊗ B)(C ⊗ D) = AC ⊗ BD.

(3.70)

(3.71)

(3.72)

(3.73)

(3.74)

These properties are all easy to see by using equation (3.69) to express the
(i, j) element of the matrix on either side of the equation, taking into account
the size of the matrices involved. For example, in the ﬁrst equation, if A is
n × m and B is p × q, the (i, j) element on the left-hand side is

aA[i/p]+1, [j/q]+1bBi−p[i/p], j−q[i/q]

74

3 Basic Properties of Matrices

and that on the right-hand side is

abA[i/p]+1, [j/q]+1Bi−p[i/p], j−q[i/q].

They are all this easy! Hence, they are Exercise 3.6.

Another property of the Kronecker product of square matrices is

tr(A ⊗ B) = tr(A)tr(B).

(3.75)

This is true because the trace of the product is merely the sum of all possible
products of the diagonal elements of the individual matrices.

The Kronecker product and the vec function often ﬁnd uses in the same
application. For example, an n × m normal random matrix X with parameters
M, Σ, and Ψ can be expressed in terms of an ordinary np-variate normal
random variable Y = vec(X) with parameters vec(M) and Σ⊗Ψ. (We discuss
matrix random variables brieﬂy on page 168. For a fuller discussion, the reader
is referred to a text on matrix random variables such as Carmeli, 1983.)

A relationship between the vec function and Kronecker multiplication is

vec(ABC) = (CT ⊗ A)vec(B)

(3.76)

for matrices A, B, and C that are conformable for the multiplication indicated.

The Dot Product or the Inner Product of Matrices

Another product of two matrices of the same shape is deﬁned as the sum of
the dot products of the vectors formed from the columns of one matrix with
vectors formed from the corresponding columns of the other matrix; that is,
if a1, . . . , am are the columns of A and b1, . . . , bm are the columns of B, then
the dot product of A and B, denoted ⟨A, B⟩, is

⟨A, B⟩ =

aT
j bj.

m!j=1

(3.77)

If A ̸= 0, ⟨A, A⟩ > 0, and ⟨0, A⟩ = ⟨A, 0⟩ = ⟨0, 0⟩ = 0.
⟨A, B⟩ = ⟨B, A⟩.
⟨sA, B⟩ = s⟨A, B⟩, for a scalar s.
⟨(A + B), C⟩ = ⟨A, C⟩ + ⟨B, C⟩.

For conformable matrices A, B, and C, we can easily conﬁrm that this
product satisﬁes the general properties of an inner product listed on page 15:
•
•
•
•
We also call this inner product of matrices the dot product of the matrices. (As
in the case of the dot product of vectors, the dot product of matrices deﬁned
over the complex ﬁeld is not an inner product because the ﬁrst property listed
above does not hold.)

As with any inner product (restricted to objects in the ﬁeld of the reals),

its value is a real number. Thus the matrix dot product is a mapping

3.2 Multiplication of Matrices

75

IRn×m × IRn×m $→ IR.

The dot product of the matrices A and B with the same shape is denoted by
A · B, or ⟨A, B⟩, just like the dot product of vectors.

We see from the deﬁnition above that the dot product of matrices satisﬁes

which could alternatively be taken as the deﬁnition.

⟨A, B⟩ = tr(ATB),
Rewriting the deﬁnition of ⟨A, B⟩ as"m
j=1"n
⟨A, B⟩ = ⟨AT, BT⟩.

i=1 aijbij, we see that

(3.78)

(3.79)

Like any inner product, dot products of matrices obey the Cauchy-Schwarz

inequality (see inequality (2.10), page 16),

⟨A, B⟩ ≤ ⟨A, A⟩

1

2⟨B, B⟩

1
2 ,

(3.80)

with equality holding only if A = 0 or B = sA for some scalar s.

In Section 2.1.8, we deﬁned orthogonality and orthonormality of two or
more vectors in terms of dot products. We can likewise deﬁne an orthogonal
binary relationship between two matrices in terms of dot products of matrices.
We say the matrices A and B of the same shape are orthogonal to each other
if

⟨A, B⟩ = 0.

(3.81)
From equations (3.78) and (3.79) we see that the matrices A and B are or-
thogonal to each other if and only if ATB and BTA are hollow (that is, they
have 0s in all diagonal positions). We also use the term “orthonormal” to refer
to matrices that are orthogonal to each other and for which each has a dot
product with itself of 1. In Section 3.7, we will deﬁne orthogonality as a unary
property of matrices. The term “orthogonal”, when applied to matrices, gen-
erally refers to that property rather than the binary property we have deﬁned
here.

On page 48 we identiﬁed a vector space of matrices and deﬁned a basis
for the space IRn×m. If {U1, . . . , Uk} is a basis set for M ⊂ IRn×m, with the
property that ⟨Ui, Uj⟩ = 0 for i ̸= j and ⟨Ui, Ui⟩ = 1, and A is an n × m
matrix, with the Fourier expansion

A =

k!i=1

ciUi,

(3.82)

we have, analogous to equation (2.37) on page 29,

76

3 Basic Properties of Matrices

ci = ⟨A, Ui⟩.

(3.83)

The ci have the same properties (such as the Parseval identity, equation (2.38),
for example) as the Fourier coeﬃcients in any orthonormal expansion. Best
approximations within M can also be expressed as truncations of the sum
in equation (3.82) as in equation (2.41). The objective of course is to reduce
the truncation error. (The norms in Parseval’s identity and in measuring the
goodness of an approximation are matrix norms in this case. We discuss matrix
norms in Section 3.9 beginning on page 128.)

3.3 Matrix Rank and the Inverse of a
Full Rank Matrix

The linear dependence or independence of the vectors forming the rows or
columns of a matrix is an important characteristic of the matrix.

The maximum number of linearly independent vectors (those forming ei-
ther the rows or the columns) is called the rank of the matrix. We use the
notation

rank(A)

to denote the rank of the matrix A. (We have used the term “rank” before to
denote dimensionality of an array. “Rank” as we have just deﬁned it applies
only to a matrix or to a set of vectors, and this is by far the more common
meaning of the word. The meaning is clear from the context, however.)

Because multiplication by a nonzero scalar does not change the linear

independence of vectors, for the scalar a with a ̸= 0, we have

rank(aA) = rank(A).

(3.84)
From results developed in Section 2.1, we see that for the n × m matrix
(3.85)

rank(A) ≤ min(n, m).

A,

Row Rank and Column Rank

We have deﬁned matrix rank in terms of numbers of linearly independent rows
or columns. This is because the number of linearly independent rows is the
same as the number of linearly independent columns. Although we may use
the terms “row rank” or “column rank”, the single word “rank” is suﬃcient
because they are the same. To see this, assume we have an n × m matrix A
and that there are exactly p linearly independent rows and exactly q linearly
independent columns. We can permute the rows and columns of the matrix
so that the ﬁrst p rows are linearly independent rows and the ﬁrst q columns
are linearly independent and the remaining rows or columns are linearly de-
pendent on the ﬁrst ones. (Recall that applying the same permutation to all

3.3 Matrix Rank and the Inverse of a Matrix

77

of the elements of each vector in a set of vectors does not change the linear
dependencies over the set.) After these permutations, we have a matrix B
with submatrices W , X, Y , and Z,

(3.86)

Yn−p×q Zn−p×m−q, ,

B =+ Wp×q Xp×m−q
where the rows of R = [W|X] correspond to p linearly independent m-vectors
Y , correspond to q linearly independent n-vectors.
and the columns of C =+ W
Without loss of generality, we can assume p ≤ q. Now, if p < q, it must be
the case that the columns of W are linearly dependent because there are q of
them, but they have only p elements. Therefore, there is some q-vector a such
that W a = 0. Now, since the rows of R are the full set of linearly independent
rows, any row in [Y |Z] can be expressed as a linear combination of the rows
of R, and any row in Y can be expressed as a linear combination of the rows
of W . This means, for some n−p × p matrix T , that Y = T W . In this case,
however, Ca = 0. But this contradicts the assumption that the columns of
C are linearly independent; therefore it cannot be the case that p < q. We
conclude therefore that p = q; that is, that the maximum number of linearly
independent rows is the same as the maximum number of linearly independent
columns.

Because the row rank, the column rank, and the rank of A are all the

same, we have

rank(A) = dim(V(A)),

rank(AT) = rank(A),

(3.87)

(3.88)

(3.89)
(Note, of course, that in general V(AT) ̸= V(A); the orders of the vector spaces
are possibly diﬀerent.)

dim(V(AT)) = dim(V(A)).

Full Rank Matrices

If the rank of a matrix is the same as its smaller dimension, we say the matrix
is of full rank. In the case of a nonsquare matrix, we may say the matrix is
of full row rank or full column rank just to emphasize which is the smaller
number.

If a matrix is not of full rank, we say it is rank deﬁcient and deﬁne the
rank deﬁciency as the diﬀerence between its smaller dimension and its rank.
A full rank matrix that is square is called nonsingular, and one that is not

nonsingular is called singular.

78

3 Basic Properties of Matrices

A square matrix that is either row or column diagonally dominant is non-

singular. The proof of this is Exercise 3.8. (It’s easy!)

A positive deﬁnite matrix is nonsingular. The proof of this is Exercise 3.9.

Later in this section, we will identify additional properties of square full
rank matrices. (For example, they have inverses and their determinants are
nonzero.)

Rank of Elementary Operator Matrices and Matrix Products
Involving Them

Because within any set of rows of an elementary operator matrix (see Sec-
tion 3.2.3), for some given column, only one of those rows contains a nonzero
element, the elementary operator matrices are all obviously of full rank (with
the proviso that a ̸= 0 in Ep(a)).
Furthermore, the rank of the product of any given matrix with an elemen-
tary operator matrix is the same as the rank of the given matrix. To see this,
consider each type of elementary operator matrix in turn. For a given matrix
A, the set of rows of EpqA is the same as the set of rows of A; hence, the rank
of EpqA is the same as the rank of A. Likewise, the set of columns of AEpq
is the same as the set of columns of A; hence, again, the rank of AEpq is the
same as the rank of A.

The set of rows of Ep(a)A for a ̸= 0 is the same as the set of rows of A,
except for one, which is a nonzero scalar multiple of the corresponding row
of A; therefore, the rank of Ep(a)A is the same as the rank of A. Likewise,
the set of columns of AEp(a) is the same as the set of columns of A, except
for one, which is a nonzero scalar multiple of the corresponding row of A;
therefore, again, the rank of AEp(a) is the same as the rank of A.

Finally, the set of rows of Epq(a)A for a ̸= 0 is the same as the set of
rows of A, except for one, which is a nonzero scalar multiple of some row of
A added to the corresponding row of A; therefore, the rank of Epq(a)A is the
same as the rank of A. Likewise, we conclude that the rank of AEpq(a) is the
same as the rank of A.

We therefore have that if P and Q are the products of elementary operator

matrices,

(3.90)
On page 88, we will extend this result to products by any full rank matrices.

rank(P AQ) = rank(A).

3.3.1 The Rank of Partitioned Matrices, Products
of Matrices, and Sums of Matrices

The partitioning in equation (3.86) leads us to consider partitioned matrices
in more detail.

3.3 Matrix Rank and the Inverse of a Matrix

79

Rank of Partitioned Matrices and Submatrices

Let the matrix A be partitioned as

A =+ A11 A12
A21 A22, ,

where any pair of submatrices in a column or row may be null (that is, where
for example, it may be the case that A = [A11|A12]). Then the number of
linearly independent rows of A must be at least as great as the number of
linearly independent rows of [A11|A12] and the number of linearly independent
rows of [A21|A22]. By the properties of subvectors in Section 2.1.1, the number
of linearly independent rows of [A11|A12] must be at least as great as the
number of linearly independent rows of A11 or A21. We could go through a
similar argument relating to the number of linearly independent columns and
arrive at the inequality

rank(Aij) ≤ rank(A).

(3.91)

Furthermore, we see that

rank(A) ≤ rank([A11|A12]) + rank([A21|A22])

(3.92)

because rank(A) is the number of linearly independent columns of A, which
is less than or equal to the number of linearly independent rows of [A11|A12]
plus the number of linearly independent rows of [A12|A22]. Likewise, we have
(3.93)

rank(A) ≤ rank1+ A11

A21,2 + rank1+ A12

A22,2 .

In a similar manner, by merely counting the number of independent rows,

we see that, if

then

and, if

then

V)[A11|A12]T* ⊥ V)[A21|A22]T* ,

rank(A) = rank([A11|A12]) + rank([A21|A22]);

V1+ A11

A21,2 ⊥ V1+ A12

A22,2 ,
A21,2 + rank1+ A12

A22,2 .

rank(A) = rank1+ A11

(3.94)

(3.95)

80

3 Basic Properties of Matrices

An Upper Bound on the Rank of Products of Matrices

The rank of the product of two matrices is less than or equal to the lesser of
the ranks of the two:

rank(AB) ≤ min(rank(A), rank(B)).

(3.96)
We can show this by separately considering two cases for the n × k matrix A
and the k × m matrix B. In one case, we assume k is at least as large as n
and n ≤ m, and in the other case we assume k < n ≤ m. In both cases, we
represent the rows of AB as k linear combinations of the rows of B.
From equation (3.96), we see that the rank of an outer product matrix

(that is, a matrix formed as the outer product of two vectors) is 1.

Equation (3.96) provides a useful upper bound on rank(AB). In Sec-

tion 3.3.8, we will develop a lower bound on rank(AB).

An Upper and a Lower Bound on the Rank of Sums of Matrices

The rank of the sum of two matrices is less than or equal to the sum of their
ranks; that is,

rank(A + B) ≤ rank(A) + rank(B).

(3.97)

We can see this by observing that

A + B = [A|B]+ I
I, ,

and so rank(A + B) ≤ rank([A|B]) by equation (3.96), which in turn is ≤
rank(A) + rank(B) by equation (3.92).
Using inequality (3.97) and the fact that rank(−B) = rank(B), we write
rank(A− B) ≤ rank(A) + rank(B), and so, replacing A in (3.97) by A + B, we
have rank(A) ≤ rank(A+B)+rank(B), or rank(A+B) ≥ rank(A)−rank(B).
By a similar procedure, we get rank(A + B) ≥ rank(B) − rank(A), or

rank(A + B) ≥ |rank(A) − rank(B)|.

(3.98)

3.3.2 Full Rank Partitioning

As we saw above, the matrix W in the partitioned B in equation (3.86) is
square; in fact, it is r × r, where r is the rank of B:
Yn−r×r Zn−r×m−r, .

B =+ Wr×r Xr×m−r

(3.99)

This is called a full rank partitioning of B.

The matrix B in equation (3.99) has a very special property: the full set
of linearly independent rows are the ﬁrst r rows, and the full set of linearly
independent columns are the ﬁrst r columns.

3.3 Matrix Rank and the Inverse of a Matrix

81

Any rank r matrix can be put in the form of equation (3.99) by using
permutation matrices as in equation (3.45), assuming that r ≥ 1. That is, if
A is a nonzero matrix, there is a matrix of the form of B above that has the
same rank. For some permutation matrices Eπ1 and Eπ2,

B = Eπ1AEπ2.

(3.100)

The inverses of these permutations coupled with the full rank partitioning of
B form a full rank partitioning of the original matrix A.

For a square matrix of rank r, this kind of partitioning implies that there
is a full rank r × r principal submatrix, and the principal submatrix formed
by including any of the remaining diagonal elements is singular. The princi-
pal minor formed from the full rank principal submatrix is nonzero, but if
the order of the matrix is greater than r, a principal minor formed from a
submatrix larger than r × r is zero.
The partitioning in equation (3.99) is of general interest, and we will use
this type of partitioning often. We express an equivalent partitioning of a
transformed matrix in equation (3.113) below.

The same methods as above can be used to form a full rank square subma-
trix of any order less than or equal to the rank. That is, if the n × m matrix
A is of rank r and q ≤ r, we can form
Eπr AEπc =+ Sq×q

Un−q×r Vn−q×m−q, ,

Tq×m−q

(3.101)

where S is of rank q.

It is obvious that the rank of a matrix can never exceed its smaller dimen-
sion (see the discussion of linear independence on page 10). Whether or not
a matrix has more rows than columns, the rank of the matrix is the same as
the dimension of the column space of the matrix. (As we have just seen, the
dimension of the column space is necessarily the same as the dimension of the
row space, but the order of the column space is diﬀerent from the order of the
row space unless the matrix is square.)

3.3.3 Full Rank Matrices and Matrix Inverses

We have already seen that full rank matrices have some important properties.
In this section, we consider full rank matrices and matrices that are their
Cayley multiplicative inverses.

Solutions of Linear Equations

Important applications of vectors and matrices involve systems of linear equa-
tions:

82

3 Basic Properties of Matrices

a11x1 +··· + a1mxm

...

an1x1 +··· + anmxm

...

?= b1
...
?= bn

(3.102)

or

Ax ?= b.

(3.103)
In this system, A is called the coeﬃcient matrix. An x that satisﬁes this
system of equations is called a solution to the system. For given A and b, a
solution may or may not exist. From equation (3.59), a solution exists if and
only if the n-vector b is in the k-dimensional column space of A, where k ≤ m.
A system for which a solution exists is said to be consistent; otherwise, it is
inconsistent.

(3.104)

(3.105)

(3.106)

(3.107)

We note that if Ax = b, for any conformable y,
yTAx = 0 ⇐⇒ yTb = 0.

Consistent Systems
A linear system An×mx = b is consistent if and only if

rank([A| b]) = rank(A).

We can see this by recognizing that the space spanned by the columns of A
is the same as that spanned by the columns of A and the vector b; therefore
b must be a linear combination of the columns of A. Furthermore, the linear
combination is the solution to the system Ax = b. (Note, of course, that it is
not necessary that it be a unique linear combination.)

Equation (3.105) is equivalent to the condition
[A| b]y = 0 ⇔ Ay = 0.

A special case that yields equation (3.105) for any b is

rank(An×m) = n,

and so if A is of full row rank, the system is consistent regardless of the value
of b. In this case, of course, the number of rows of A must be no greater than
the number of columns (by inequality (3.85)). A square system in which A is
nonsingular is clearly consistent.

A generalization of the linear system Ax = b is AX = B, where B is an
n × k matrix. This is the same as k systems Ax1 = b1, . . . , Axk = bk, where
the x1 and the bi are the columns of the respective matrices. Such a system
is consistent if each of the Axi = bi systems is consistent. Consistency of
AX = B, as above, is the condition for a solution in X to exist.

We discuss methods for solving linear systems in Section 3.5 and in Chap-
ter 6. In the next section, we consider a special case of n × n (square) A when
equation (3.107) is satisﬁed (that is, when A is nonsingular).

